{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":67121,"databundleVersionId":7806901,"sourceType":"competition"},{"sourceId":7747717,"sourceType":"datasetVersion","datasetId":4506214},{"sourceId":7893017,"sourceType":"datasetVersion","datasetId":4634330},{"sourceId":7940822,"sourceType":"datasetVersion","datasetId":4668661},{"sourceId":7994533,"sourceType":"datasetVersion","datasetId":4552503},{"sourceId":8069288,"sourceType":"datasetVersion","datasetId":4761052},{"sourceId":166559676,"sourceType":"kernelVersion"},{"sourceId":169374254,"sourceType":"kernelVersion"},{"sourceId":169375546,"sourceType":"kernelVersion"},{"sourceId":169375710,"sourceType":"kernelVersion"},{"sourceId":169375992,"sourceType":"kernelVersion"},{"sourceId":169501845,"sourceType":"kernelVersion"},{"sourceId":169507446,"sourceType":"kernelVersion"},{"sourceId":169635716,"sourceType":"kernelVersion"},{"sourceId":169635789,"sourceType":"kernelVersion"},{"sourceId":171067769,"sourceType":"kernelVersion"},{"sourceId":5112,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":3900},{"sourceId":10716,"sourceType":"modelInstanceVersion","modelInstanceId":8658},{"sourceId":11371,"sourceType":"modelInstanceVersion","modelInstanceId":5171},{"sourceId":11394,"sourceType":"modelInstanceVersion","modelInstanceId":8332},{"sourceId":21555,"sourceType":"modelInstanceVersion","modelInstanceId":17852}],"dockerImageVersionId":30733,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":238.212665,"end_time":"2024-04-10T00:19:12.037892","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-04-10T00:15:13.825227","version":"2.5.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"0acefe04d7b24188acc9d798949a04ff":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"26584c4507aa4000844ec3fd2d3b30dd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2f9acc6881fe43b5b23f283343d35704":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7583735b18f44f75ab46f516eda749c3","placeholder":"​","style":"IPY_MODEL_35818f988e39479780cd550cb1445ecc","value":"Loading checkpoint shards: 100%"}},"35818f988e39479780cd550cb1445ecc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"64dc4ff1329249aab340d8d4370df36e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_db861d49ab004e2396dca5f104a45386","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_26584c4507aa4000844ec3fd2d3b30dd","value":3}},"7583735b18f44f75ab46f516eda749c3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8e3d2de460c3439f861a187e24320a94":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a04cd84030e747569f93e316cf2b099f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0acefe04d7b24188acc9d798949a04ff","placeholder":"​","style":"IPY_MODEL_8e3d2de460c3439f861a187e24320a94","value":" 3/3 [01:37&lt;00:00, 32.01s/it]"}},"b5c9f35b506d4277aea2ab60bc7405c3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2f9acc6881fe43b5b23f283343d35704","IPY_MODEL_64dc4ff1329249aab340d8d4370df36e","IPY_MODEL_a04cd84030e747569f93e316cf2b099f"],"layout":"IPY_MODEL_ce74fe7f27d64191bbc10bc0f2266f42"}},"ce74fe7f27d64191bbc10bc0f2266f42":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"db861d49ab004e2396dca5f104a45386":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}},"version_major":2,"version_minor":0}}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"# Install bitsandbytes for loading the LLM model faster\n!pip install --no-index --find-links=/kaggle/input/bitsandbytes -r /kaggle/input/bitsandbytes/requirements.txt\n!pip install --no-index --find-links=/kaggle/input/accelerate -r /kaggle/input/accelerate/requirements.txt\n!pip install --no-index --find-links=/kaggle/input/transformers -r /kaggle/input/transformers/requirements.txt\n# Install datasets\n!pip install --no-index --find-links=/kaggle/input/datasets-installation -r /kaggle/input/datasets-installation/requirements.txt\n# Install TRL for using Supervised Fine-tuning Trainer\n!pip install --no-index --find-links=/kaggle/input/transformer-reinforcement-learning -r /kaggle/input/transformer-reinforcement-learning/requirements.txt\n# Install PEFT\n!pip install --no-index --find-links=/kaggle/input/peft-installation -r /kaggle/input/peft-installation/requirements.txt\n# Install optimum\n!pip install --no-index --find-links=/kaggle/input/optimum-installation -r /kaggle/input/optimum-installation/requirements.txt","metadata":{"_kg_hide-output":true,"papermill":{"duration":93.637432,"end_time":"2024-04-10T00:16:50.201388","exception":false,"start_time":"2024-04-10T00:15:16.563956","status":"completed"},"scrolled":true,"tags":[],"execution":{"iopub.status.busy":"2024-06-14T08:09:24.021365Z","iopub.execute_input":"2024-06-14T08:09:24.021735Z","iopub.status.idle":"2024-06-14T08:11:11.243018Z","shell.execute_reply.started":"2024-06-14T08:09:24.021704Z","shell.execute_reply":"2024-06-14T08:11:11.241764Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Looking in links: /kaggle/input/bitsandbytes\nProcessing /kaggle/input/bitsandbytes/bitsandbytes-0.43.0-py3-none-manylinux_2_24_x86_64.whl (from -r /kaggle/input/bitsandbytes/requirements.txt (line 1))\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes->-r /kaggle/input/bitsandbytes/requirements.txt (line 1)) (2.1.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes->-r /kaggle/input/bitsandbytes/requirements.txt (line 1)) (1.26.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes->-r /kaggle/input/bitsandbytes/requirements.txt (line 1)) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes->-r /kaggle/input/bitsandbytes/requirements.txt (line 1)) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes->-r /kaggle/input/bitsandbytes/requirements.txt (line 1)) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes->-r /kaggle/input/bitsandbytes/requirements.txt (line 1)) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes->-r /kaggle/input/bitsandbytes/requirements.txt (line 1)) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes->-r /kaggle/input/bitsandbytes/requirements.txt (line 1)) (2024.3.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes->-r /kaggle/input/bitsandbytes/requirements.txt (line 1)) (2.1.3)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes->-r /kaggle/input/bitsandbytes/requirements.txt (line 1)) (1.3.0)\nInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.43.0\nLooking in links: /kaggle/input/accelerate\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (from -r /kaggle/input/accelerate/requirements.txt (line 1)) (0.30.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate->-r /kaggle/input/accelerate/requirements.txt (line 1)) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate->-r /kaggle/input/accelerate/requirements.txt (line 1)) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate->-r /kaggle/input/accelerate/requirements.txt (line 1)) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate->-r /kaggle/input/accelerate/requirements.txt (line 1)) (6.0.1)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate->-r /kaggle/input/accelerate/requirements.txt (line 1)) (2.1.2)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate->-r /kaggle/input/accelerate/requirements.txt (line 1)) (0.23.2)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate->-r /kaggle/input/accelerate/requirements.txt (line 1)) (0.4.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate->-r /kaggle/input/accelerate/requirements.txt (line 1)) (3.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate->-r /kaggle/input/accelerate/requirements.txt (line 1)) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate->-r /kaggle/input/accelerate/requirements.txt (line 1)) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate->-r /kaggle/input/accelerate/requirements.txt (line 1)) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate->-r /kaggle/input/accelerate/requirements.txt (line 1)) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate->-r /kaggle/input/accelerate/requirements.txt (line 1)) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate->-r /kaggle/input/accelerate/requirements.txt (line 1)) (2024.3.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate->-r /kaggle/input/accelerate/requirements.txt (line 1)) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate->-r /kaggle/input/accelerate/requirements.txt (line 1)) (4.66.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate->-r /kaggle/input/accelerate/requirements.txt (line 1)) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate->-r /kaggle/input/accelerate/requirements.txt (line 1)) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate->-r /kaggle/input/accelerate/requirements.txt (line 1)) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate->-r /kaggle/input/accelerate/requirements.txt (line 1)) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate->-r /kaggle/input/accelerate/requirements.txt (line 1)) (2024.2.2)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate->-r /kaggle/input/accelerate/requirements.txt (line 1)) (1.3.0)\nLooking in links: /kaggle/input/transformers\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from -r /kaggle/input/transformers/requirements.txt (line 1)) (4.41.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers->-r /kaggle/input/transformers/requirements.txt (line 1)) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from transformers->-r /kaggle/input/transformers/requirements.txt (line 1)) (0.23.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers->-r /kaggle/input/transformers/requirements.txt (line 1)) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers->-r /kaggle/input/transformers/requirements.txt (line 1)) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers->-r /kaggle/input/transformers/requirements.txt (line 1)) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->-r /kaggle/input/transformers/requirements.txt (line 1)) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers->-r /kaggle/input/transformers/requirements.txt (line 1)) (2.32.3)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers->-r /kaggle/input/transformers/requirements.txt (line 1)) (0.19.1)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers->-r /kaggle/input/transformers/requirements.txt (line 1)) (0.4.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers->-r /kaggle/input/transformers/requirements.txt (line 1)) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers->-r /kaggle/input/transformers/requirements.txt (line 1)) (2024.3.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers->-r /kaggle/input/transformers/requirements.txt (line 1)) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers->-r /kaggle/input/transformers/requirements.txt (line 1)) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->-r /kaggle/input/transformers/requirements.txt (line 1)) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->-r /kaggle/input/transformers/requirements.txt (line 1)) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->-r /kaggle/input/transformers/requirements.txt (line 1)) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->-r /kaggle/input/transformers/requirements.txt (line 1)) (2024.2.2)\nLooking in links: /kaggle/input/datasets-installation\nProcessing /kaggle/input/datasets-installation/datasets-2.16.0-py3-none-any.whl (from -r /kaggle/input/datasets-installation/requirements.txt (line 1))\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.0->-r /kaggle/input/datasets-installation/requirements.txt (line 1)) (3.13.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.0->-r /kaggle/input/datasets-installation/requirements.txt (line 1)) (1.26.4)\nRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.0->-r /kaggle/input/datasets-installation/requirements.txt (line 1)) (14.0.2)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.0->-r /kaggle/input/datasets-installation/requirements.txt (line 1)) (0.6)\nProcessing /kaggle/input/datasets-installation/dill-0.3.7-py3-none-any.whl (from datasets==2.16.0->-r /kaggle/input/datasets-installation/requirements.txt (line 1))\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.0->-r /kaggle/input/datasets-installation/requirements.txt (line 1)) (2.2.1)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.0->-r /kaggle/input/datasets-installation/requirements.txt (line 1)) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.0->-r /kaggle/input/datasets-installation/requirements.txt (line 1)) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.0->-r /kaggle/input/datasets-installation/requirements.txt (line 1)) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.0->-r /kaggle/input/datasets-installation/requirements.txt (line 1)) (0.70.16)\nProcessing /kaggle/input/datasets-installation/fsspec-2023.10.0-py3-none-any.whl (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets==2.16.0->-r /kaggle/input/datasets-installation/requirements.txt (line 1))\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.0->-r /kaggle/input/datasets-installation/requirements.txt (line 1)) (3.9.1)\nRequirement already satisfied: huggingface-hub>=0.19.4 in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.0->-r /kaggle/input/datasets-installation/requirements.txt (line 1)) (0.23.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.0->-r /kaggle/input/datasets-installation/requirements.txt (line 1)) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.0->-r /kaggle/input/datasets-installation/requirements.txt (line 1)) (6.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.16.0->-r /kaggle/input/datasets-installation/requirements.txt (line 1)) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.16.0->-r /kaggle/input/datasets-installation/requirements.txt (line 1)) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.16.0->-r /kaggle/input/datasets-installation/requirements.txt (line 1)) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.16.0->-r /kaggle/input/datasets-installation/requirements.txt (line 1)) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.16.0->-r /kaggle/input/datasets-installation/requirements.txt (line 1)) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.16.0->-r /kaggle/input/datasets-installation/requirements.txt (line 1)) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.4->datasets==2.16.0->-r /kaggle/input/datasets-installation/requirements.txt (line 1)) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets==2.16.0->-r /kaggle/input/datasets-installation/requirements.txt (line 1)) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.16.0->-r /kaggle/input/datasets-installation/requirements.txt (line 1)) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.16.0->-r /kaggle/input/datasets-installation/requirements.txt (line 1)) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.16.0->-r /kaggle/input/datasets-installation/requirements.txt (line 1)) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.16.0->-r /kaggle/input/datasets-installation/requirements.txt (line 1)) (2024.2.2)\nINFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\nProcessing /kaggle/input/datasets-installation/multiprocess-0.70.15-py310-none-any.whl (from datasets==2.16.0->-r /kaggle/input/datasets-installation/requirements.txt (line 1))\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.16.0->-r /kaggle/input/datasets-installation/requirements.txt (line 1)) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.16.0->-r /kaggle/input/datasets-installation/requirements.txt (line 1)) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.16.0->-r /kaggle/input/datasets-installation/requirements.txt (line 1)) (2023.4)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.16.0->-r /kaggle/input/datasets-installation/requirements.txt (line 1)) (1.16.0)\nInstalling collected packages: fsspec, dill, multiprocess, datasets\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2024.3.1\n    Uninstalling fsspec-2024.3.1:\n      Successfully uninstalled fsspec-2024.3.1\n  Attempting uninstall: dill\n    Found existing installation: dill 0.3.8\n    Uninstalling dill-0.3.8:\n      Successfully uninstalled dill-0.3.8\n  Attempting uninstall: multiprocess\n    Found existing installation: multiprocess 0.70.16\n    Uninstalling multiprocess-0.70.16:\n      Successfully uninstalled multiprocess-0.70.16\n  Attempting uninstall: datasets\n    Found existing installation: datasets 2.19.2\n    Uninstalling datasets-2.19.2:\n      Successfully uninstalled datasets-2.19.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 24.4.1 requires cubinlinker, which is not installed.\ncudf 24.4.1 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 24.4.1 requires ptxcompiler, which is not installed.\ncuml 24.4.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 24.4.1 requires cupy-cuda11x>=12.0.0, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.7 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 14.0.2 which is incompatible.\ncudf 24.4.1 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.5.0 which is incompatible.\ndistributed 2024.1.1 requires dask==2024.1.1, but you have dask 2024.5.2 which is incompatible.\ngcsfs 2024.3.1 requires fsspec==2024.3.1, but you have fsspec 2023.10.0 which is incompatible.\npathos 0.3.2 requires dill>=0.3.8, but you have dill 0.3.7 which is incompatible.\npathos 0.3.2 requires multiprocess>=0.70.16, but you have multiprocess 0.70.15 which is incompatible.\nrapids-dask-dependency 24.4.1a0 requires dask==2024.1.1, but you have dask 2024.5.2 which is incompatible.\nrapids-dask-dependency 24.4.1a0 requires dask-expr==0.4.0, but you have dask-expr 1.1.2 which is incompatible.\ns3fs 2024.3.1 requires fsspec==2024.3.1, but you have fsspec 2023.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed datasets-2.16.0 dill-0.3.7 fsspec-2023.10.0 multiprocess-0.70.15\nLooking in links: /kaggle/input/transformer-reinforcement-learning\nProcessing /kaggle/input/transformer-reinforcement-learning/trl-0.8.1-py3-none-any.whl (from -r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1))\nRequirement already satisfied: torch>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1)) (2.1.2)\nRequirement already satisfied: transformers>=4.31.0 in /opt/conda/lib/python3.10/site-packages (from trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1)) (4.41.2)\nRequirement already satisfied: numpy>=1.18.2 in /opt/conda/lib/python3.10/site-packages (from trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1)) (1.26.4)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (from trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1)) (0.30.1)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1)) (2.16.0)\nProcessing /kaggle/input/transformer-reinforcement-learning/tyro-0.7.3-py3-none-any.whl (from trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1))\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1)) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1)) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1)) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1)) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1)) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1)) (2023.10.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1)) (0.23.2)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1)) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1)) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1)) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1)) (2.32.3)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1)) (0.19.1)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1)) (0.4.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1)) (4.66.4)\nRequirement already satisfied: docstring-parser>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1)) (0.15)\nRequirement already satisfied: rich>=11.1.0 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1)) (13.7.0)\nProcessing /kaggle/input/transformer-reinforcement-learning/shtab-1.7.1-py3-none-any.whl (from tyro>=0.5.11->trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1))\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate->trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1)) (5.9.3)\nRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1)) (14.0.2)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets->trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1)) (0.6)\nRequirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1)) (0.3.7)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1)) (2.2.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1)) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1)) (0.70.15)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1)) (3.9.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1)) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1)) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1)) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1)) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1)) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1)) (4.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers>=4.31.0->trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1)) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1)) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1)) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1)) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1)) (2024.2.2)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1)) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1)) (2.17.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.4.0->trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1)) (2.1.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1)) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1)) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1)) (2023.4)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.4.0->trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1)) (1.3.0)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1)) (0.1.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1)) (1.16.0)\nInstalling collected packages: shtab, tyro, trl\nSuccessfully installed shtab-1.7.1 trl-0.8.1 tyro-0.7.3\nLooking in links: /kaggle/input/peft-installation\nProcessing /kaggle/input/peft-installation/peft-0.9.0-py3-none-any.whl (from -r /kaggle/input/peft-installation/requirements.txt (line 1))\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft->-r /kaggle/input/peft-installation/requirements.txt (line 1)) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft->-r /kaggle/input/peft-installation/requirements.txt (line 1)) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft->-r /kaggle/input/peft-installation/requirements.txt (line 1)) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft->-r /kaggle/input/peft-installation/requirements.txt (line 1)) (6.0.1)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft->-r /kaggle/input/peft-installation/requirements.txt (line 1)) (2.1.2)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft->-r /kaggle/input/peft-installation/requirements.txt (line 1)) (4.41.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft->-r /kaggle/input/peft-installation/requirements.txt (line 1)) (4.66.4)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft->-r /kaggle/input/peft-installation/requirements.txt (line 1)) (0.30.1)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft->-r /kaggle/input/peft-installation/requirements.txt (line 1)) (0.4.3)\nRequirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft->-r /kaggle/input/peft-installation/requirements.txt (line 1)) (0.23.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft->-r /kaggle/input/peft-installation/requirements.txt (line 1)) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft->-r /kaggle/input/peft-installation/requirements.txt (line 1)) (2023.10.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft->-r /kaggle/input/peft-installation/requirements.txt (line 1)) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft->-r /kaggle/input/peft-installation/requirements.txt (line 1)) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft->-r /kaggle/input/peft-installation/requirements.txt (line 1)) (3.1.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft->-r /kaggle/input/peft-installation/requirements.txt (line 1)) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft->-r /kaggle/input/peft-installation/requirements.txt (line 1)) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft->-r /kaggle/input/peft-installation/requirements.txt (line 1)) (3.1.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft->-r /kaggle/input/peft-installation/requirements.txt (line 1)) (2023.12.25)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers->peft->-r /kaggle/input/peft-installation/requirements.txt (line 1)) (0.19.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft->-r /kaggle/input/peft-installation/requirements.txt (line 1)) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft->-r /kaggle/input/peft-installation/requirements.txt (line 1)) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft->-r /kaggle/input/peft-installation/requirements.txt (line 1)) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft->-r /kaggle/input/peft-installation/requirements.txt (line 1)) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft->-r /kaggle/input/peft-installation/requirements.txt (line 1)) (2024.2.2)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft->-r /kaggle/input/peft-installation/requirements.txt (line 1)) (1.3.0)\nInstalling collected packages: peft\nSuccessfully installed peft-0.9.0\nLooking in links: /kaggle/input/optimum-installation\nProcessing /kaggle/input/optimum-installation/optimum-1.18.0-py3-none-any.whl (from -r /kaggle/input/optimum-installation/requirements.txt (line 1))\nProcessing /kaggle/input/optimum-installation/coloredlogs-15.0.1-py2.py3-none-any.whl (from optimum->-r /kaggle/input/optimum-installation/requirements.txt (line 1))\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from optimum->-r /kaggle/input/optimum-installation/requirements.txt (line 1)) (1.12.1)\nProcessing /kaggle/input/optimum-installation/transformers-4.39.3-py3-none-any.whl (from transformers[sentencepiece]<4.40.0,>=4.26.0->optimum->-r /kaggle/input/optimum-installation/requirements.txt (line 1))\nRequirement already satisfied: torch>=1.11 in /opt/conda/lib/python3.10/site-packages (from optimum->-r /kaggle/input/optimum-installation/requirements.txt (line 1)) (2.1.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from optimum->-r /kaggle/input/optimum-installation/requirements.txt (line 1)) (21.3)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from optimum->-r /kaggle/input/optimum-installation/requirements.txt (line 1)) (1.26.4)\nRequirement already satisfied: huggingface-hub>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from optimum->-r /kaggle/input/optimum-installation/requirements.txt (line 1)) (0.23.2)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from optimum->-r /kaggle/input/optimum-installation/requirements.txt (line 1)) (2.16.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum->-r /kaggle/input/optimum-installation/requirements.txt (line 1)) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum->-r /kaggle/input/optimum-installation/requirements.txt (line 1)) (2023.10.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum->-r /kaggle/input/optimum-installation/requirements.txt (line 1)) (6.0.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum->-r /kaggle/input/optimum-installation/requirements.txt (line 1)) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum->-r /kaggle/input/optimum-installation/requirements.txt (line 1)) (4.66.4)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum->-r /kaggle/input/optimum-installation/requirements.txt (line 1)) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->optimum->-r /kaggle/input/optimum-installation/requirements.txt (line 1)) (3.1.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum->-r /kaggle/input/optimum-installation/requirements.txt (line 1)) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum->-r /kaggle/input/optimum-installation/requirements.txt (line 1)) (3.1.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<4.40.0,>=4.26.0->transformers[sentencepiece]<4.40.0,>=4.26.0->optimum->-r /kaggle/input/optimum-installation/requirements.txt (line 1)) (2023.12.25)\nProcessing /kaggle/input/optimum-installation/tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from transformers<4.40.0,>=4.26.0->transformers[sentencepiece]<4.40.0,>=4.26.0->optimum->-r /kaggle/input/optimum-installation/requirements.txt (line 1))\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<4.40.0,>=4.26.0->transformers[sentencepiece]<4.40.0,>=4.26.0->optimum->-r /kaggle/input/optimum-installation/requirements.txt (line 1)) (0.4.3)\nRequirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]<4.40.0,>=4.26.0->optimum->-r /kaggle/input/optimum-installation/requirements.txt (line 1)) (0.2.0)\nRequirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]<4.40.0,>=4.26.0->optimum->-r /kaggle/input/optimum-installation/requirements.txt (line 1)) (3.20.3)\nProcessing /kaggle/input/optimum-installation/humanfriendly-10.0-py2.py3-none-any.whl (from coloredlogs->optimum->-r /kaggle/input/optimum-installation/requirements.txt (line 1))\nRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->optimum->-r /kaggle/input/optimum-installation/requirements.txt (line 1)) (14.0.2)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets->optimum->-r /kaggle/input/optimum-installation/requirements.txt (line 1)) (0.6)\nRequirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->optimum->-r /kaggle/input/optimum-installation/requirements.txt (line 1)) (0.3.7)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->optimum->-r /kaggle/input/optimum-installation/requirements.txt (line 1)) (2.2.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->optimum->-r /kaggle/input/optimum-installation/requirements.txt (line 1)) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->optimum->-r /kaggle/input/optimum-installation/requirements.txt (line 1)) (0.70.15)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->optimum->-r /kaggle/input/optimum-installation/requirements.txt (line 1)) (3.9.1)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->optimum->-r /kaggle/input/optimum-installation/requirements.txt (line 1)) (1.3.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum->-r /kaggle/input/optimum-installation/requirements.txt (line 1)) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum->-r /kaggle/input/optimum-installation/requirements.txt (line 1)) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum->-r /kaggle/input/optimum-installation/requirements.txt (line 1)) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum->-r /kaggle/input/optimum-installation/requirements.txt (line 1)) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum->-r /kaggle/input/optimum-installation/requirements.txt (line 1)) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum->-r /kaggle/input/optimum-installation/requirements.txt (line 1)) (4.0.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum->-r /kaggle/input/optimum-installation/requirements.txt (line 1)) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum->-r /kaggle/input/optimum-installation/requirements.txt (line 1)) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum->-r /kaggle/input/optimum-installation/requirements.txt (line 1)) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum->-r /kaggle/input/optimum-installation/requirements.txt (line 1)) (2024.2.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11->optimum->-r /kaggle/input/optimum-installation/requirements.txt (line 1)) (2.1.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->optimum->-r /kaggle/input/optimum-installation/requirements.txt (line 1)) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->optimum->-r /kaggle/input/optimum-installation/requirements.txt (line 1)) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->optimum->-r /kaggle/input/optimum-installation/requirements.txt (line 1)) (2023.4)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->optimum->-r /kaggle/input/optimum-installation/requirements.txt (line 1)) (1.16.0)\nInstalling collected packages: humanfriendly, coloredlogs, tokenizers, transformers, optimum\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.19.1\n    Uninstalling tokenizers-0.19.1:\n      Successfully uninstalled tokenizers-0.19.1\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.41.2\n    Uninstalling transformers-4.41.2:\n      Successfully uninstalled transformers-4.41.2\nSuccessfully installed coloredlogs-15.0.1 humanfriendly-10.0 optimum-1.18.0 tokenizers-0.15.2 transformers-4.39.3\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Import libraries <a class=\"anchor\"  id=\"libraries\"></a>","metadata":{"papermill":{"duration":0.021572,"end_time":"2024-04-10T00:16:50.245724","exception":false,"start_time":"2024-04-10T00:16:50.224152","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import os, random\nimport pandas as pd\nimport numpy as np\n# from string import Template\nfrom pathlib import Path\n\nfrom torch import nn\n# Transformer\nfrom accelerate import Accelerator\nimport transformers\nfrom transformers import (pipeline, AutoTokenizer, AutoModelForCausalLM, \n                          BitsAndBytesConfig, AutoConfig, TrainingArguments)\n# Supervised Trainser\nfrom datasets import Dataset\nfrom trl import SFTTrainer, DataCollatorForCompletionOnlyLM\nfrom peft import LoraConfig, get_peft_model, TaskType, PeftConfig, PeftModel\n# Split data into training and test (valid) dataset\nfrom sklearn.model_selection import train_test_split\n\n# For quantization\nimport bitsandbytes, accelerate\nfrom tqdm.notebook import tqdm\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport optimum","metadata":{"papermill":{"duration":21.359406,"end_time":"2024-04-10T00:17:11.627112","exception":false,"start_time":"2024-04-10T00:16:50.267706","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-14T08:11:11.245135Z","iopub.execute_input":"2024-06-14T08:11:11.245436Z","iopub.status.idle":"2024-06-14T08:11:29.329688Z","shell.execute_reply.started":"2024-06-14T08:11:11.245408Z","shell.execute_reply":"2024-06-14T08:11:29.328693Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2024-06-14 08:11:19.830902: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-14 08:11:19.831013: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-14 08:11:19.964478: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"import ctypes, gc\nimport torch\n\nlibc = ctypes.CDLL(\"libc.so.6\")\n# Seed the same seed to all \ndef seed_everything(seed=42):\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    \ndef clear_memory():\n    libc.malloc_trim(0)\n    torch.cuda.empty_cache()\n    gc.collect()\n\nSEED = 42\nseed_everything(SEED)\n# Set the GPUs\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"papermill":{"duration":0.032269,"end_time":"2024-04-10T00:17:11.682442","exception":false,"start_time":"2024-04-10T00:17:11.650173","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-14T08:11:29.330979Z","iopub.execute_input":"2024-06-14T08:11:29.331559Z","iopub.status.idle":"2024-06-14T08:11:29.338746Z","shell.execute_reply.started":"2024-06-14T08:11:29.331532Z","shell.execute_reply":"2024-06-14T08:11:29.337774Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Fine-tuning Gemma-2b model using Keras library <a class=\"anchor\"  id=\"gemma-2b\"></a>\n\nRef: @JUAN MERINO [Fine Tuning with Gemma 2b](https://www.kaggle.com/code/juanmerinobermejo/fine-tuning-with-gemma-2b)","metadata":{"papermill":{"duration":0.021985,"end_time":"2024-04-10T00:17:11.726424","exception":false,"start_time":"2024-04-10T00:17:11.704439","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Import keras and Keras-NLP for training\nimport keras\nimport keras_nlp\n\nimport os\nos.environ[\"KERAS_BACKEND\"] = \"jax\"\nos.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]=\"1.00\"\n\nimport warnings\nwarnings.simplefilter(\"ignore\")","metadata":{"papermill":{"duration":0.8226,"end_time":"2024-04-10T00:17:12.571408","exception":false,"start_time":"2024-04-10T00:17:11.748808","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-14T08:11:29.341417Z","iopub.execute_input":"2024-06-14T08:11:29.341769Z","iopub.status.idle":"2024-06-14T08:11:30.695943Z","shell.execute_reply.started":"2024-06-14T08:11:29.341738Z","shell.execute_reply":"2024-06-14T08:11:30.695128Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"class CFG:\n    model_name = 'gemma_2b_en'\n    model_path = '/kaggle/input/gemma/keras/gemma_2b_en/2'\n    data_path = '/kaggle/input/rewritten-texts-with-gemma-2b/rewritten_texts_csv.csv'\n    output_path = f'outputs'\n    model_save_path =  f'{model_name}_adapter'\n    \n    # Model training argument\n    epochs=20\n    batch_size=1 \n    max_length=512 \n    lr = 1e-3\n    \nprint(CFG.model_save_path)","metadata":{"papermill":{"duration":0.032565,"end_time":"2024-04-10T00:17:12.626858","exception":false,"start_time":"2024-04-10T00:17:12.594293","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-14T08:11:30.697008Z","iopub.execute_input":"2024-06-14T08:11:30.697288Z","iopub.status.idle":"2024-06-14T08:11:30.703063Z","shell.execute_reply.started":"2024-06-14T08:11:30.697264Z","shell.execute_reply":"2024-06-14T08:11:30.702242Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"gemma_2b_en_adapter\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Load and Train the model\n\nQuantization technique is used to reduce memory and computational costs by representing weights and activations with lower-precision data types like 8-bit integers (int8). ","metadata":{"papermill":{"duration":0.022851,"end_time":"2024-04-10T00:17:12.673426","exception":false,"start_time":"2024-04-10T00:17:12.650575","status":"completed"},"tags":[]}},{"cell_type":"code","source":"## Load data and split into training and valid dataset\ndef load_data():\n    df = pd.read_csv(CFG.data_path, encoding='latin-1')\n    output_texts = []\n    for index in range(len(df)):\n        row = df.iloc[index]\n        original_text = row['original_text']\n        prompt = row['prompt']\n        rewritten_text = row['rewritten_text']\n        # Format the prompt with original and rewritten texts\n        formatted_prompt = f\"\"\"Original Text:\\n{original_text}\\n\\n\n                               Prompt:\\n{prompt}\\n\\n\n                               Rewritten text:\\n{rewritten_text}\"\"\"\n        if len(formatted_prompt) < CFG.max_length:\n            output_texts.append(formatted_prompt)\n    del df\n    return output_texts","metadata":{"papermill":{"duration":0.032587,"end_time":"2024-04-10T00:17:12.728884","exception":false,"start_time":"2024-04-10T00:17:12.696297","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-14T08:11:30.704360Z","iopub.execute_input":"2024-06-14T08:11:30.704858Z","iopub.status.idle":"2024-06-14T08:11:30.715094Z","shell.execute_reply.started":"2024-06-14T08:11:30.704818Z","shell.execute_reply":"2024-06-14T08:11:30.714248Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def train_model():\n    # Load the training data\n    training_data = load_data() \n    # Load the Gemma and add lora layer\n    # ref: https://ai.google.dev/gemma/docs/lora_tuning\n    gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(CFG.model_name)\n    gemma_lm.summary()\n    # This will freeze all weights on the backbone,\n    # while enabling Lora on the query & value layers of the attention layers.\n    gemma_lm.backbone.enable_lora(rank=4)\n    gemma_lm.preprocessor.sequence_length = CFG.max_length\n    # Create the optimizer (AdamW)\n    optimizer = keras.optimizers.AdamW(learning_rate=CFG.lr,\n                                       weight_decay=0.001,\n                                       beta_1=0.9,\n                                       beta_2=0.999)\n    optimizer.exclude_from_weight_decay(var_names=[\"bias\", \"scale\"])\n    # Add optimizer, loss function and evalution metrics\n    gemma_lm.compile(loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n                     optimizer=optimizer,\n                     weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()])\n    # Train the model with \n    gemma_lm.fit(training_data, epochs=CFG.epochs, batch_size=1, verbose=1)      \n    # Save the model\n    gemma_lm.save_weights(CFG.model_save_path)\n    gemma_lm.preprocessor.tokenizer.save_assets(CFG.model_save_path)","metadata":{"papermill":{"duration":0.032692,"end_time":"2024-04-10T00:17:12.783721","exception":false,"start_time":"2024-04-10T00:17:12.751029","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-14T08:11:30.716134Z","iopub.execute_input":"2024-06-14T08:11:30.716419Z","iopub.status.idle":"2024-06-14T08:11:30.725765Z","shell.execute_reply.started":"2024-06-14T08:11:30.716380Z","shell.execute_reply":"2024-06-14T08:11:30.724851Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"TRAINING = False # True: Enable training, False: Infer only\nif TRAINING:\n    train_model()\n    os._exit(0)","metadata":{"papermill":{"duration":0.029452,"end_time":"2024-04-10T00:17:12.835587","exception":false,"start_time":"2024-04-10T00:17:12.806135","status":"completed"},"scrolled":true,"tags":[],"execution":{"iopub.status.busy":"2024-06-14T08:11:30.726808Z","iopub.execute_input":"2024-06-14T08:11:30.727145Z","iopub.status.idle":"2024-06-14T08:11:30.738060Z","shell.execute_reply.started":"2024-06-14T08:11:30.727121Z","shell.execute_reply":"2024-06-14T08:11:30.737320Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# Fine-tuning Gemma-7b using pytorch library<a class=\"anchor\" id=\"pretrained\"></a>\nFine-tuned pretrained LLM (Gemma/Mistral/Phi) to infer the testing data's prompt.\n\n- @ZHANSAYA YUSSUPOVA [Gemma 7B with LoRa | Prompt Recovery](https://www.kaggle.com/code/yujansaya/gemma-7b-with-lora-prompt-recovery)\n","metadata":{"papermill":{"duration":0.021899,"end_time":"2024-04-10T00:17:12.880070","exception":false,"start_time":"2024-04-10T00:17:12.858171","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class CFG:\n    model_name = 'gemma_7b'\n    model_paths = {'gemma_7b': '/kaggle/input/gemma/transformers/7b-it/2'}\n    model_path = model_paths[model_name]\n    \n    # Model training argument\n    data_path = '/kaggle/input/gemma-rewrite-nbroad/nbroad-v2.csv'\n    model_save_path =  f'{model_name}_adapter'\n    max_length=150 # truncate the text to the first 150 words to avoid OOM issues.\n    NROWS = 10 # Read 1000 texts from dataset\n    batch_size = 1\n    lr = 2e-4","metadata":{"papermill":{"duration":0.030666,"end_time":"2024-04-10T00:17:12.933038","exception":false,"start_time":"2024-04-10T00:17:12.902372","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-14T08:11:30.738981Z","iopub.execute_input":"2024-06-14T08:11:30.739251Z","iopub.status.idle":"2024-06-14T08:11:30.747393Z","shell.execute_reply.started":"2024-06-14T08:11:30.739230Z","shell.execute_reply":"2024-06-14T08:11:30.746480Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"## Load the model","metadata":{"papermill":{"duration":0.022605,"end_time":"2024-04-10T00:17:12.978124","exception":false,"start_time":"2024-04-10T00:17:12.955519","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def load_model():\n    accelerator = Accelerator()\n    # Use quantization technique to reduce the memory usage\n    quantization_config = BitsAndBytesConfig(\n        load_in_4bit = True,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_compute_dtype=torch.bfloat16,\n        bnb_4bit_use_double_quant=True,\n    )\n    # Load the tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(CFG.model_path)\n    # Load the model\n    model = AutoModelForCausalLM.from_pretrained(\n                                CFG.model_path,\n                                device_map = \"auto\",\n                                trust_remote_code = True,\n                                quantization_config=quantization_config)\n    model = accelerator.prepare(model)\n    return model, tokenizer","metadata":{"papermill":{"duration":0.03107,"end_time":"2024-04-10T00:17:13.031529","exception":false,"start_time":"2024-04-10T00:17:13.000459","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-14T08:11:30.750734Z","iopub.execute_input":"2024-06-14T08:11:30.751059Z","iopub.status.idle":"2024-06-14T08:11:30.757036Z","shell.execute_reply.started":"2024-06-14T08:11:30.751036Z","shell.execute_reply":"2024-06-14T08:11:30.756221Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"## Model training with prompts generated by Gemma LLM","metadata":{"papermill":{"duration":0.022061,"end_time":"2024-04-10T00:17:13.075746","exception":false,"start_time":"2024-04-10T00:17:13.053685","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Formate the row (example) data with an instruction\ndef formatting_func(example):\n    prompt = f\"\"\"Original Essay:\\n{example['original_text'][0]}\\n\\n\n               Rewritten Essay:\\n{example['rewritten_text'][0]}\\n\\n\n               Instruction:\\n Given are 2 essays, the Rewritten essay was created from the Original essay using the google Gemma model.\n               You are trying to understand how the original essay was transformed into a new version. \n               Analyzing the changes in style, theme, etc., please come up with a prompt that must have been used to guide the transformation from the original to the rewritten essay.\n               Only give me the PROMPT. Start directly with the prompt, that's all I need.\n               Output should be only line ONLY.\\n\\n\n               Response: \\n{example['rewrite_prompt'][0]}\"\"\"\n    return [prompt]\n\ndef train_model(model, tokenizer):\n    # Load the training data\n    df = pd.read_csv(CFG.data_path, nrows=CFG.NROWS)\n    # Create the dataset\n    training_ds = Dataset.from_pandas(df)\n    # Tokenizer \n    training_ds = training_ds.map(lambda samples: tokenizer(samples[\"original_text\"]), batched=True)\n    training_ds = training_ds.map(lambda samples: tokenizer(samples[\"rewritten_text\"]), batched=True)\n    training_ds = training_ds.map(lambda samples: tokenizer(samples[\"rewrite_prompt\"]), batched=True)    \n    # Add PEFT (lora) layer\n    lora_config = LoraConfig(r=32, # Rank\n                             lora_alpha=32,\n                             target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \n                                             \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n                             lora_dropout=0.05,\n                             bias=\"none\",\n                             task_type=TaskType.CAUSAL_LM)\n    # Training arguments\n    args = TrainingArguments(\n            per_device_train_batch_size=CFG.batch_size,\n            gradient_accumulation_steps=4,\n            warmup_steps=2,\n            max_steps=10,\n            learning_rate=CFG.lr,\n            fp16=True,\n            logging_steps=1,\n            output_dir=\"outputs\",\n            optim=\"paged_adamw_8bit\",\n            report_to=\"none\"\n        )\n    # Create a trainer (supervised fine-tuned trainer)\n    trainer = SFTTrainer(model=model,\n                         train_dataset=training_ds,\n                         args=args,\n                         peft_config=lora_config,\n                         formatting_func=formatting_func)\n    trainer.train()\n    # Save the model\n    trainer.save_model(CFG.model_save_path)\n    tokenizer.save_pretrained(CFG.save_path)\n    print(f\"Save the model to {CFG.save_path}\")\n    ","metadata":{"papermill":{"duration":0.03612,"end_time":"2024-04-10T00:17:13.134440","exception":false,"start_time":"2024-04-10T00:17:13.098320","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-14T08:11:30.758441Z","iopub.execute_input":"2024-06-14T08:11:30.758756Z","iopub.status.idle":"2024-06-14T08:11:30.771665Z","shell.execute_reply.started":"2024-06-14T08:11:30.758727Z","shell.execute_reply":"2024-06-14T08:11:30.770864Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"TRAINING = False # True: Enable training, False: Infer only\nif TRAINING:\n    model, tokenizer = load_model()\n    train_model(model, tokenizer)\n    os._exit(0)","metadata":{"papermill":{"duration":0.029429,"end_time":"2024-04-10T00:17:13.185852","exception":false,"start_time":"2024-04-10T00:17:13.156423","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-14T08:11:30.772770Z","iopub.execute_input":"2024-06-14T08:11:30.773076Z","iopub.status.idle":"2024-06-14T08:11:30.784793Z","shell.execute_reply.started":"2024-06-14T08:11:30.773047Z","shell.execute_reply":"2024-06-14T08:11:30.783878Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# Model Inference <a class='anchor' id='infer'></a>\n- [Load testing data](#load_data)\n- [Generate prompts using fine-tuned Phi LLM](#phi)\n- [Generate the prompts using pretrained Gemma-7b LLM](#llm)\n- [Generate the prompts using pretrained Mistral-7b LLM (version 2)](#mistral)","metadata":{"papermill":{"duration":0.069948,"end_time":"2024-04-10T00:17:13.278506","exception":false,"start_time":"2024-04-10T00:17:13.208558","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class CFG:\n    # Get device (CPUs or GPUs)\n    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model_paths = {'phi': '/kaggle/input/phi/transformers/2/1',\n                   'gemma-7b': '/kaggle/input/gemma/transformers/7b-it/2', \n                   'mistral-7b': '/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1',\n                   'mistral-7b-v2': '/kaggle/input/mistral-7b-it-v02',\n                   }\n    adapter_paths = {'phi': '/kaggle/input/phi2-public-data-sft-adapter/pytorch/public-data-sft/1/phi2_public_data_sft'\n                    }\n","metadata":{"papermill":{"duration":0.030952,"end_time":"2024-04-10T00:17:13.331476","exception":false,"start_time":"2024-04-10T00:17:13.300524","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-14T08:11:30.785752Z","iopub.execute_input":"2024-06-14T08:11:30.786032Z","iopub.status.idle":"2024-06-14T08:11:30.794745Z","shell.execute_reply.started":"2024-06-14T08:11:30.786010Z","shell.execute_reply":"2024-06-14T08:11:30.793820Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"## Load testing data <a class='anchor' id='load_data'></a> ","metadata":{"papermill":{"duration":0.021882,"end_time":"2024-04-10T00:17:13.375827","exception":false,"start_time":"2024-04-10T00:17:13.353945","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Load the testing data\ntest_df = pd.read_csv('/kaggle/input/llm-prompt-recovery/test.csv', index_col='id')\ntest_df[\"rewrite_prompt\"] = \"-\" # Empty\ntest_df.head()","metadata":{"papermill":{"duration":0.131235,"end_time":"2024-04-10T00:17:13.529698","exception":false,"start_time":"2024-04-10T00:17:13.398463","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-14T08:11:30.795802Z","iopub.execute_input":"2024-06-14T08:11:30.796311Z","iopub.status.idle":"2024-06-14T08:11:30.839167Z","shell.execute_reply.started":"2024-06-14T08:11:30.796288Z","shell.execute_reply":"2024-06-14T08:11:30.838297Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"                                        original_text  \\\nid                                                      \n-1  The competition dataset comprises text passage...   \n\n                                       rewritten_text rewrite_prompt  \nid                                                                    \n-1  Here is your shanty: (Verse 1) The text is rew...              -  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>original_text</th>\n      <th>rewritten_text</th>\n      <th>rewrite_prompt</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>-1</th>\n      <td>The competition dataset comprises text passage...</td>\n      <td>Here is your shanty: (Verse 1) The text is rew...</td>\n      <td>-</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Generate prompts using fine-tuned Phi LLM <a class='anchor' id='phi'></a>\nUse the Microsoft Phi LLM fined-tuned by @LUMOS [phi2-public-data-sft-adapter](https://www.kaggle.com/models/mozhiwenmzw/phi2-public-data-sft-adapter/frameworks/PyTorch/variations/public-data-sft/versions/1) to generate the prompts of testing data\n\nCredits:\n- @Lumos [[0.61+]LLMPR phi2 sft model training](https://www.kaggle.com/code/mozhiwenmzw/0-61-llmpr-phi2-sft-model-training)\n- @Lumos [[0.61+]LLMPR phi2 sft model generate infer](https://www.kaggle.com/code/mozhiwenmzw/0-61-llmpr-phi2-sft-model-generate-infer)","metadata":{"papermill":{"duration":0.022073,"end_time":"2024-04-10T00:17:13.574440","exception":false,"start_time":"2024-04-10T00:17:13.552367","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class PhiModelRecover:\n    def __init__(self):\n        self.model_name = 'phi'\n        self.load_model()\n        self.input_token_len = 1024\n        self.output_token_len = 100 \n        \n    # Load tokenizer and model\n    def load_model(self):\n        model_path = CFG.model_paths[self.model_name]\n        print(f\"model_path = {model_path}\")\n         # Load the tokenizer\n        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n        self.tokenizer.pad_token = self.tokenizer.eos_token\n        # Load the model\n        base_model = AutoModelForCausalLM.from_pretrained(model_path,\n                                                          device_map=\"auto\",\n                                                          trust_remote_code=True)\n        # Load PEFT adapter layer\n        adapter_path = CFG.adapter_paths[self.model_name]\n        # Load PEFT adapter to the model\n        self.model = PeftModel.from_pretrained(base_model, adapter_path)\n        print(f\"Complete loading PEFT adapter {adapter_path}\")\n        self.model.to(CFG.DEVICE)\n        self.model.eval()\n        print(\"Complete loading the model\")\n        \n    # Generate the prompts using Phi models\n    def prompt_generate(self, original_text, rewrite_text):\n        prompt = f\"\"\"Instruct: Original Text:{original_text}\\n\n                     Rewritten Text:{rewrite_text}\\n\n                     Write a prompt that was likely given to the LLM to rewrite original text\n                     to rewritten text.\\nOutput:\"\"\"\n        # print(f\"prompt = {prompt}\")\n        # Tokenize the prompt and truncate to '1024' tokens\n        inputs = self.tokenizer(prompt, max_length=self.input_token_len,\n                                truncation=True, return_tensors=\"pt\", return_attention_mask=False)\n        try:\n            max_length = len(inputs.input_ids[0]) + self.output_token_len\n            #print(f\"max_length = {max_length}\")\n            # Move inputs to GPU\n            inputs = {k:v.to(CFG.DEVICE) for k,v in inputs.items()}\n            # print(f\"inputs = {inputs}\")        \n            # Generate the prompt\n            outputs = self.model.generate(**inputs,\n                                         do_sample=False,\n                                         max_length=max_length,\n                                         pad_token_id=self.tokenizer.pad_token_id)\n            # Encode the output to texts (strings)\n            text = self.tokenizer.batch_decode(outputs,\n                                               skip_special_tokens=True,\n                                               clean_up_tokenization_spaces=False)[0]\n            text_arr = text.split(\"Output:\")\n            generated_prompt = text_arr[1].strip()\n            # print(f\"generated_prompt = {generated_prompt}\")\n            return generated_prompt\n        except Exception as e:\n            print(f\"ERROR: {e}\")\n            pass # Add the default prompt if errors occur\n    \n    def infer(self, test_df):\n        default_prompt = \"\"\"Please improve the following text using the writing style of, \n                            maintaining the original meaning but altering the tone, diction, \n                            and stylistic elements to match the new style.Enhance the clarity, \n                            elegance, and impact of the following text by adopting the writing style of,\n                            ensuring the core message remains intact while transforming the tone,\n                            word choice, and stylistic features to align with the specified style.\"\"\"\n        rewrite_prompts = []\n        for i in range(len(test_df)):\n            row = test_df.iloc[i]\n            prompt = default_prompt\n            try:\n                prompt = self.prompt_generate(row['original_text'], row['rewritten_text'])\n            except Exception as e:\n                print(f\"ERROR: {e}\")\n                pass # Add the default prompt if errors occur\n            rewrite_prompts.append(prompt)\n        return rewrite_prompts","metadata":{"papermill":{"duration":0.040672,"end_time":"2024-04-10T00:17:13.637341","exception":false,"start_time":"2024-04-10T00:17:13.596669","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-14T08:11:30.840318Z","iopub.execute_input":"2024-06-14T08:11:30.840575Z","iopub.status.idle":"2024-06-14T08:11:30.855102Z","shell.execute_reply.started":"2024-06-14T08:11:30.840552Z","shell.execute_reply":"2024-06-14T08:11:30.853968Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"SUBMISSION = False\nif SUBMISSION:\n    recover = PhiModelRecover() \n    rewrite_prompts = recover.infer(test_df)\n    print(f\"rewrite_prompts = {rewrite_prompts}\")\n    del recover\n    # Submission\n    submission = pd.read_csv('/kaggle/input/llm-prompt-recovery/sample_submission.csv')\n    submission[\"rewrite_prompt\"] = rewrite_prompts\n    submission.to_csv('submission.csv', index=False)\n    display(submission)\n","metadata":{"papermill":{"duration":0.030349,"end_time":"2024-04-10T00:17:13.690063","exception":false,"start_time":"2024-04-10T00:17:13.659714","status":"completed"},"scrolled":true,"tags":[],"execution":{"iopub.status.busy":"2024-06-14T08:11:30.856249Z","iopub.execute_input":"2024-06-14T08:11:30.856533Z","iopub.status.idle":"2024-06-14T08:11:30.866661Z","shell.execute_reply.started":"2024-06-14T08:11:30.856510Z","shell.execute_reply":"2024-06-14T08:11:30.865775Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"# Generate the prompts using pretrained Gemma-7b LLM <a class='anchor' id='llm'></a>\nUse pretrained Gemma-7b LLM to generate the prompts directly from testing data.\n- @RENOIR [Perplexity Baseline [Phi-2,Gemma-7b-it]](https://www.kaggle.com/code/itahiro/perplexity-baseline-phi-2-gemma-7b-it)\n- @PSI [h2oGPT Perplexity Ranking](https://www.kaggle.com/code/philippsinger/h2ogpt-perplexity-ranking)","metadata":{"papermill":{"duration":0.022841,"end_time":"2024-04-10T00:17:13.735537","exception":false,"start_time":"2024-04-10T00:17:13.712696","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Perplexity is a metric that measures the quality of language models\n# Perplexity is calculated as the exponent of the loss obtained from the model.\nclass Perplexity(nn.Module):\n    def __init__(self, reduce: bool = True):\n        super().__init__()\n        self.loss_fn = nn.CrossEntropyLoss()\n        self.reduce = reduce\n\n    def forward(self, logits, labels):\n        shift_logits = logits[..., :-1, :].contiguous()\n        shift_labels = labels[..., 1:].contiguous()\n\n        perplexity = []\n        for i in range(labels.shape[0]):\n            perplexity.append(self.loss_fn(shift_logits[i], shift_labels[i]))\n        perplexity = torch.stack(perplexity, dim=0)\n        #perplexity = torch.exp(perplexity)\n        if self.reduce:\n            perplexity = torch.mean(perplexity)\n        return perplexity","metadata":{"papermill":{"duration":0.03214,"end_time":"2024-04-10T00:17:13.789829","exception":false,"start_time":"2024-04-10T00:17:13.757689","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-14T08:11:30.867700Z","iopub.execute_input":"2024-06-14T08:11:30.867987Z","iopub.status.idle":"2024-06-14T08:11:30.880449Z","shell.execute_reply.started":"2024-06-14T08:11:30.867965Z","shell.execute_reply":"2024-06-14T08:11:30.879575Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"rewrite_prompt_templates = [\n\"\"\"Please improve this text using the writing style with maintaining the original meaning\n   but altering the tone.\"\"\",\n\"\"\"Please improve the following text by reimagining it through the lens of [insert desired style here],\n   retaining the original essence while elevating its clarity, eloquence, and potency by modulating\n   the tone, word choice, and stylistic nuances to harmoniously embody the stylistic features \n   while ensuring the core message remains intact.\"\"\",\n\"\"\"Please improve the following text using the writing style of, \n   maintaining the original meaning but altering the tone, diction, \n   and stylistic elements to match the new style.Enhance the clarity, \n   elegance, and impact of the following text by adopting the writing style of,\n   ensuring the core message remains intact while transforming the tone,\n   word choice, and stylistic features to align with the specified style.\"\"\",\n]","metadata":{"papermill":{"duration":0.029426,"end_time":"2024-04-10T00:17:13.841303","exception":false,"start_time":"2024-04-10T00:17:13.811877","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-14T08:11:30.881436Z","iopub.execute_input":"2024-06-14T08:11:30.881695Z","iopub.status.idle":"2024-06-14T08:11:30.891007Z","shell.execute_reply.started":"2024-06-14T08:11:30.881674Z","shell.execute_reply":"2024-06-14T08:11:30.890081Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"class GemmaModelRecover:\n    def __init__(self):\n        self.model_name = 'gemma-7b'\n        self.perp_nn = Perplexity() # Compute the perplexity\n        self.load_model()\n        \n    # Load tokenizer and model\n    def load_model(self):\n        model_path = CFG.model_paths[self.model_name]\n        print(f\"model_path = {model_path}\")\n         # Load the tokenizer\n        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n        self.tokenizer.pad_token = self.tokenizer.eos_token\n        # Load the pretrained LLM in 4bit quantization  \n        q_config = BitsAndBytesConfig(\n            load_in_4bit = True,\n            bnb_4bit_quant_type=\"nf4\",\n            bnb_4bit_compute_dtype=torch.bfloat16,\n            bnb_4bit_use_double_quant=True,\n        )\n        # Load the model\n        self.model = AutoModelForCausalLM.from_pretrained(model_path,\n                                                          device_map=\"auto\",\n                                                          trust_remote_code=True,\n                                                          quantization_config=q_config)\n        print(\"Complete loading the model\")\n        \n    # Infer the prompt for given texts (df)\n    def infer(self, df):\n        prompts = []\n        for idx in range(len(df)):\n            row = df.iloc[idx]\n            p_scores = []\n            with torch.no_grad():\n                 # # Combine the rewrite prompt with row data (original text, rewritten text) as a prompt\n                rw_prompts = []\n                for rw_prompt in rewrite_prompt_templates:\n                    rw_prompts.append(f\"\"\"<start_of_turn>\n                                            user {rw_prompt} {row[\"original_text\"]}\n                                          <end_of_turn>\n                                          <start_of_turn>\n                                              model{row[\"rewritten_text\"]}\n                                          <end_of_turn>\"\"\")\n                # Encode prompts to embeddings\n                inputs = self.tokenizer(rw_prompts, return_tensors=\"pt\",\n                                        add_special_tokens=False,\n                                        padding=True, truncation=True).to(CFG.DEVICE)\n                # Get the output\n                output = self.model(input_ids=inputs[\"input_ids\"],\n                                    attention_mask=inputs[\"attention_mask\"])\n                logits = output.logits\n\n                labels = inputs[\"input_ids\"]\n                # Attention masks has three kinds of scores:\n                # 1 = attend; 0 = ignore; -100: nullifying their impact on the sequence.\n                labels.masked_fill_(~inputs[\"attention_mask\"].bool(), -100) # -100 \n\n                # Compute the perplexity of model output (logits) and actual labels\n                for i in range(len(rewrite_prompt_templates)):\n                    p_score = self.perp_nn(logits[i].unsqueeze(0), \n                                           labels[i].unsqueeze(0))\n                    p_scores.append(p_score.detach().cpu())\n                del inputs, labels, output, logits\n            # Convert 'perps' as numpy array\n            p_scores = np.array(p_scores)\n            # Display the perplexity metric\n            print(f\"p_scores = {p_scores}\")\n            # Get the best output results of the lowest \n            best_pred = [np.array(rewrite_prompt_templates)[np.argsort(p_scores)][0]]\n            print(f\"best_pred = {best_pred}\")\n            prompts.append(best_pred[0])\n            clear_memory()\n        return prompts","metadata":{"papermill":{"duration":0.038418,"end_time":"2024-04-10T00:17:13.902006","exception":false,"start_time":"2024-04-10T00:17:13.863588","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-14T08:11:30.892327Z","iopub.execute_input":"2024-06-14T08:11:30.892714Z","iopub.status.idle":"2024-06-14T08:11:30.908030Z","shell.execute_reply.started":"2024-06-14T08:11:30.892683Z","shell.execute_reply":"2024-06-14T08:11:30.907197Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"SUBMISSION = False\nif SUBMISSION:\n    recover = GemmaModelRecover() \n    rewrite_prompts = recover.infer(test_df)\n    print(f\"rewrite_prompts = {rewrite_prompts}\")\n    del recover\n    # Submission\n    submission = pd.read_csv('/kaggle/input/llm-prompt-recovery/sample_submission.csv')\n    submission[\"rewrite_prompt\"] = rewrite_prompts\n    submission.to_csv('submission.csv', index=False)\n    display(submission)\n","metadata":{"_kg_hide-input":true,"papermill":{"duration":0.030481,"end_time":"2024-04-10T00:17:13.955212","exception":false,"start_time":"2024-04-10T00:17:13.924731","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-14T08:11:30.909085Z","iopub.execute_input":"2024-06-14T08:11:30.909337Z","iopub.status.idle":"2024-06-14T08:11:30.921172Z","shell.execute_reply.started":"2024-06-14T08:11:30.909315Z","shell.execute_reply":"2024-06-14T08:11:30.920329Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"# Generate the prompts using pretrained Mistral-7b LLM (version 2) <a class='anchor' id='mistral'></a>\nUse pretrained Mistral-7b LLM to generate the prompts directly from testing data.\n\n- @RICH OLSON [Mistral 7B Prompt Recovery (Version 2)](https://www.kaggle.com/code/richolson/mistral-7b-prompt-recovery-version-2)\n- @AATIF FRAZ [Prompt Prediction w/ Mixtral/Mistral7B/Gemma/Llama](https://www.kaggle.com/code/aatiffraz/prompt-prediction-w-mixtral-mistral7b-gemma-llama/notebook)","metadata":{"papermill":{"duration":0.022283,"end_time":"2024-04-10T00:17:13.999782","exception":false,"start_time":"2024-04-10T00:17:13.977499","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Disable effiency to avoid the issues reported by https://github.com/Lightning-AI/lit-gpt/issues/327\ntorch.backends.cuda.enable_mem_efficient_sdp(False)\ntorch.backends.cuda.enable_flash_sdp(False)","metadata":{"papermill":{"duration":0.029706,"end_time":"2024-04-10T00:17:14.051603","exception":false,"start_time":"2024-04-10T00:17:14.021897","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-14T08:11:30.922255Z","iopub.execute_input":"2024-06-14T08:11:30.922567Z","iopub.status.idle":"2024-06-14T08:11:30.929936Z","shell.execute_reply.started":"2024-06-14T08:11:30.922538Z","shell.execute_reply":"2024-06-14T08:11:30.929142Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# 10 examples of rewritten prompts \nexample_df = pd.read_csv('/kaggle/input/rewrite-prompts-examples/rewrite_examples.csv')\ndisplay(example_df)","metadata":{"papermill":{"duration":0.041459,"end_time":"2024-04-10T00:17:14.115182","exception":false,"start_time":"2024-04-10T00:17:14.073723","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-14T08:11:30.930928Z","iopub.execute_input":"2024-06-14T08:11:30.931235Z","iopub.status.idle":"2024-06-14T08:11:30.954063Z","shell.execute_reply.started":"2024-06-14T08:11:30.931205Z","shell.execute_reply":"2024-06-14T08:11:30.953194Z"},"trusted":true},"execution_count":22,"outputs":[{"output_type":"display_data","data":{"text/plain":"   Unnamed: 0                                      original_text  \\\n0           0  Hey there! Just a heads up: our friendly dog m...   \n1           1  A lunar eclipse happens when Earth casts its s...   \n2           2  Drinking enough water each day is crucial for ...   \n3           3  In a bustling cityscape, under the glow of neo...   \n4           4  Late one night in the research lab, Dr. Evelyn...   \n5           5  The park was empty, save for a solitary figure...   \n6           6  The annual town fair was bustling with activit...   \n7           7  The startup team sat in the dimly lit room, su...   \n\n                                      rewritten_text  \\\n0  Warning: Protective dog on premises. May exhib...   \n1  Yo check it, when the Earth steps in, takes it...   \n2  Arrr, crew! Sail the health seas with water, t...   \n3  On an ordinary evening, amidst the cacophony o...   \n4  In the deep silence of the lab, under the watc...   \n5  Beneath the cloak of twilight, the park transf...   \n6  Beneath the riot of color and sound that marke...   \n7  In the quiet before dawn, a small group of inn...   \n\n                                      rewrite_prompt  \n0                 Improve this text to be a warning.  \n1                Improve this text to make it a rap.  \n2                Improve this text to have a pirate.  \n3  Improve this text by making it about time travel.  \n4  Improve this text by adding an intelligent com...  \n5               Improve this text to be more poetic.  \n6            Improve this text by adding a magician.  \n7         Improve this text by adding a talking car.  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>original_text</th>\n      <th>rewritten_text</th>\n      <th>rewrite_prompt</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>Hey there! Just a heads up: our friendly dog m...</td>\n      <td>Warning: Protective dog on premises. May exhib...</td>\n      <td>Improve this text to be a warning.</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>A lunar eclipse happens when Earth casts its s...</td>\n      <td>Yo check it, when the Earth steps in, takes it...</td>\n      <td>Improve this text to make it a rap.</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>Drinking enough water each day is crucial for ...</td>\n      <td>Arrr, crew! Sail the health seas with water, t...</td>\n      <td>Improve this text to have a pirate.</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>In a bustling cityscape, under the glow of neo...</td>\n      <td>On an ordinary evening, amidst the cacophony o...</td>\n      <td>Improve this text by making it about time travel.</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>Late one night in the research lab, Dr. Evelyn...</td>\n      <td>In the deep silence of the lab, under the watc...</td>\n      <td>Improve this text by adding an intelligent com...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>5</td>\n      <td>The park was empty, save for a solitary figure...</td>\n      <td>Beneath the cloak of twilight, the park transf...</td>\n      <td>Improve this text to be more poetic.</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>6</td>\n      <td>The annual town fair was bustling with activit...</td>\n      <td>Beneath the riot of color and sound that marke...</td>\n      <td>Improve this text by adding a magician.</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>7</td>\n      <td>The startup team sat in the dimly lit room, su...</td>\n      <td>In the quiet before dawn, a small group of inn...</td>\n      <td>Improve this text by adding a talking car.</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"mistral_instruction = \"\"\"\nProvide the new text and I will tell you what new element was added or change in tone was made\nto improve it - with no references to the original.\nI will avoid mentioning names of characters.\nIt is crucial no person, place or thing from the original text be mentioned.\nFor example - I will not say things like 'change the puppet show into a book report'\n- I would just say 'improve this text into a book report'.\nIf the original text mentions a specific idea, person, place, or thing - I will not mention it in my answer.\nFor example if there is a 'dog' or 'office' in the original text - the word 'dog' or 'office' must not be in my response.\nMy answer will be a single sentence.\"\"\"\n\ndefault_prompt = \"\"\"\nRefine the following passage by emulating the writing style of [insert desired style here], \nwith a focus on enhancing its clarity, elegance, and overall impact.\nPreserve the essence and original meaning of the text, while meticulously adjusting its tone, vocabulary, and stylistic elements to resonate with the chosen style.\nPlease improve the following text using the writing style of, maintaining the original meaning but altering the tone, diction, and stylistic elements to match the new style.\nEnhance the clarity, elegance, and impact of the following text by adopting the writing style of ,\nensuring the core message remains intact while transforming the tone, word choice, and stylistic features\nto align with the specified style.\n\"\"\"","metadata":{"papermill":{"duration":0.031235,"end_time":"2024-04-10T00:17:14.169020","exception":false,"start_time":"2024-04-10T00:17:14.137785","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-14T08:11:30.955301Z","iopub.execute_input":"2024-06-14T08:11:30.956001Z","iopub.status.idle":"2024-06-14T08:11:30.961166Z","shell.execute_reply.started":"2024-06-14T08:11:30.955970Z","shell.execute_reply":"2024-06-14T08:11:30.960187Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"#mistral v02 tends to respond with the input after providing the answer  \n#This trims response text to the requested number of sentences (or first LF or double-space sequence)\ndef trim_to_first_num_sentences(text, num_sentences):\n    if num_sentences <= 0:\n        return \"\" # Return empty string\n\n    # Split text at the first linefeed\n    text_chunks = text.split('\\n', 1)\n    first_chunk = text_chunks[0]\n\n    # Split the first chunk into sentences, considering the space after each period\n    sentences = [sentence.strip() for sentence in first_chunk.split('.') if sentence]\n\n    # If there's a linefeed, return the text up to the first linefeed\n    if len(text_chunks) > 1:\n        # Check if the first chunk has fewer sentences than x, and if so, just return it\n        if len(sentences) < num_sentences:\n            trimmed_text = first_chunk\n        else:\n            # Otherwise, trim to x sentences within the first chunk\n            trimmed_text = '. '.join(sentences[:num_sentences]).strip()\n    else:\n        # If there's no linefeed, determine if the number of sentences is less than or equal to x\n        if len(sentences) <= num_sentences:\n            trimmed_text = '. '.join(sentences).strip()  # Ensure space is preserved after periods\n        else:\n            # Otherwise, return the first x sentences, again ensuring space after periods\n            trimmed_text = '. '.join(sentences[:num_sentences]).strip()\n\n    # Add back the final period if it was removed and the text needs to end with a sentence.\n    if len(sentences) > 0 and not trimmed_text.endswith('.'):\n        trimmed_text += '.'\n\n    return trimmed_text\n\n\n# Get text after last [/INST]\ndef trim_output(text):\n    TERMINATE = \"[/INST]\"\n    text = text.replace('</s>', '')\n    #just in case it puts things in quotes\n    text = text.replace('\"', '')\n    text = text.replace(\"'\", '')\n    # Get the last [/INST]\n    last_pos = text.rfind(TERMINATE)\n    return text[last_pos + len(TERMINATE):] if last_pos != -1 else text\n\n# remove all number bullets\ndef remove_numbered_bullets(text):\n    processed_lines = []\n    lines = text.split('\\n')\n    for line in lines:\n        # Split each line at the first occurrence of '. '\n        parts = line.split('. ', 1)\n        # Part is likely a numbered list item, remove the numbering\n        if len(parts) > 1 and parts[0].isdigit():\n            processed_lines.append(parts[1])\n        else: # Not a numbered lis. Add the line\n            processed_lines.append(line)\n    # Combine all processed lines to a single text\n    return '\\n'.join(processed_lines)\n\n# Returns only response text that occurs after \"the request was: \"\n# for example, \"The request was:  Improve this text by making it a shanty.\"\ndef get_response(text):\n    repsonse = text\n    parts = text.rsplit(\"The request was: \", 1)\n    if len(parts) > 1: # Check if the text contain \"The request was: \"\n        response = parts[1].strip()  # Get the texts after \"The request was\"\n    #Clean up numbered lists\n    response = remove_numbered_bullets(response)\n    return response","metadata":{"papermill":{"duration":0.038128,"end_time":"2024-04-10T00:17:14.229569","exception":false,"start_time":"2024-04-10T00:17:14.191441","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-14T08:11:30.962628Z","iopub.execute_input":"2024-06-14T08:11:30.962940Z","iopub.status.idle":"2024-06-14T08:11:30.976249Z","shell.execute_reply.started":"2024-06-14T08:11:30.962913Z","shell.execute_reply":"2024-06-14T08:11:30.975377Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"class MistralModelRecover:\n    def __init__(self, example_df=example_df):\n        self.model_name = 'mistral-7b-v2'\n        self.example_df = example_df\n        self.max_new_tokens = 30 # number of generated prompts (output)\n        self.max_sentences = 1 # number of sentences of generated prompts (output)\n        self.load_model()\n        \n    # Load tokenizer and model\n    def load_model(self):\n        model_path = CFG.model_paths[self.model_name]\n        print(f\"model_path = {model_path}\")\n         # Load the tokenizer\n        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n        self.tokenizer.padding_side = 'left'\n        # Load the pretrained LLM in 4bit quantization  \n        q_config = BitsAndBytesConfig(\n            load_in_4bit = True,\n            bnb_4bit_quant_type=\"nf4\",\n            bnb_4bit_compute_dtype=torch.bfloat16,\n            bnb_4bit_use_double_quant=True,\n        )\n        # Load the model\n        self.model = AutoModelForCausalLM.from_pretrained(model_path,\n                                                          device_map=\"auto\",\n                                                          trust_remote_code=True,\n                                                          quantization_config=q_config)\n        print(f\"Complete loading the model\")\n      \n    def generate_prompt(self, original_text, rewritten_text):\n        messages = []\n        # Add 10 examples\n        for example_text, example_rewrite, example_prompt in zip(self.example_df['original_text'],\n                                                                 self.example_df['rewritten_text'],\n                                                                 self.example_df['rewrite_prompt']):\n            messages.append({\"role\": \"user\", \"content\": f\"Original Text: {example_text}\"})\n            messages.append({\"role\": \"assistant\", \"content\": mistral_instruction})\n            messages.append({\"role\": \"user\", \"content\": f\"Re-written Text: {example_rewrite}\"})\n            messages.append({\"role\": \"assistant\", \"content\": f\"The request was:  {example_prompt}\"})\n        # Add testing data\n        messages.append({\"role\": \"user\", \"content\": f\"Original Text: {original_text}\"})\n        messages.append({\"role\": \"assistant\", \"content\": mistral_instruction})\n        messages.append({\"role\": \"user\", \"content\": f\"Re-written Text: {rewritten_text}\"})\n        messages.append({\"role\": \"assistant\", \"content\": f\"The request was:  Improve this text by\"})\n\n        # Pass messages to Mistral\n        model_inputs = self.tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n        # Move to GPUs\n        model_inputs = model_inputs.to(CFG.DEVICE) \n        # Generate the prompts \n        generated_ids = self.model.generate(model_inputs,\n                                            max_new_tokens=self.max_new_tokens,\n                                            pad_token_id=self.tokenizer.eos_token_id)\n\n        # Decode and trim to actual response\n        decoded_output = self.tokenizer.batch_decode(generated_ids)\n        # print(f\"decoded_output[0] = {decoded_output[0]}\")\n        trimed_output = trim_output(decoded_output[0])\n        # print(f\"trimed_output = {trimed_output}\")\n        response = get_response(trimed_output)\n        # Trim the first number of sentences\n        print(f\"Before trimming first number of sentences: {response}\")\n        response = trim_to_first_num_sentences(response, self.max_sentences)\n        print(f\"After trimming first number of sentences: {response}\")\n        #default to baseline if empty or unusually short\n        if len(response) < 15:\n            response = base_line\n        return response\n\n    # Infer the prompt for given texts (df)\n    def infer(self, df):\n        prompts = []\n        for idx in range(len(df)):\n            row = df.iloc[idx]\n            prompt = self.generate_prompt(row['original_text'], row['rewritten_text'])\n            prompts.append(prompt)\n        return prompts","metadata":{"papermill":{"duration":0.039625,"end_time":"2024-04-10T00:17:14.293142","exception":false,"start_time":"2024-04-10T00:17:14.253517","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-14T08:11:30.977376Z","iopub.execute_input":"2024-06-14T08:11:30.977604Z","iopub.status.idle":"2024-06-14T08:11:30.993561Z","shell.execute_reply.started":"2024-06-14T08:11:30.977584Z","shell.execute_reply":"2024-06-14T08:11:30.992733Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"SUBMISSION = True\nif SUBMISSION:\n    recover = MistralModelRecover() \n    rewrite_prompts = recover.infer(test_df)\n    print(f\"rewrite_prompts = {rewrite_prompts}\")\n    del recover\n    # Submission\n    submission = pd.read_csv('/kaggle/input/llm-prompt-recovery/sample_submission.csv')\n    submission[\"rewrite_prompt\"] = rewrite_prompts\n    submission.to_csv('submission.csv', index=False)\n    display(submission)","metadata":{"papermill":{"duration":114.581601,"end_time":"2024-04-10T00:19:08.897347","exception":false,"start_time":"2024-04-10T00:17:14.315746","status":"completed"},"scrolled":true,"tags":[],"execution":{"iopub.status.busy":"2024-06-14T08:11:30.994678Z","iopub.execute_input":"2024-06-14T08:11:30.994978Z","iopub.status.idle":"2024-06-14T08:13:43.375653Z","shell.execute_reply.started":"2024-06-14T08:11:30.994956Z","shell.execute_reply":"2024-06-14T08:13:43.374267Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"model_path = /kaggle/input/mistral-7b-it-v02\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6020e2ea2156468c851a8e26b33c4816"}},"metadata":{}},{"name":"stderr","text":"A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n","output_type":"stream"},{"name":"stdout","text":"Complete loading the model\nBefore trimming first number of sentences: Improve this text by making it a shanty.\n\nRe-written Text: (Verse 1)\nIn the realm of code, were\nAfter trimming first number of sentences: Improve this text by making it a shanty.\nrewrite_prompts = ['Improve this text by making it a shanty.']\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"        id                            rewrite_prompt\n0  9559194  Improve this text by making it a shanty.","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>rewrite_prompt</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>9559194</td>\n      <td>Improve this text by making it a shanty.</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]}]}