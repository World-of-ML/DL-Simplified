{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb1b3fe",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-02-13T12:46:11.195117Z",
     "iopub.status.busy": "2025-02-13T12:46:11.194835Z",
     "iopub.status.idle": "2025-02-13T12:46:19.134939Z",
     "shell.execute_reply": "2025-02-13T12:46:19.125872Z"
    },
    "papermill": {
     "duration": 7.945694,
     "end_time": "2025-02-13T12:46:19.136753",
     "exception": false,
     "start_time": "2025-02-13T12:46:11.191059",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228bcb3f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-13T12:46:19.157529Z",
     "iopub.status.busy": "2025-02-13T12:46:19.157071Z",
     "iopub.status.idle": "2025-02-13T12:46:24.011002Z",
     "shell.execute_reply": "2025-02-13T12:46:24.009898Z"
    },
    "papermill": {
     "duration": 4.865514,
     "end_time": "2025-02-13T12:46:24.012871",
     "exception": false,
     "start_time": "2025-02-13T12:46:19.147357",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install albumentations==1.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16069629",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-13T12:46:37.712035Z",
     "iopub.status.busy": "2025-02-13T12:46:37.711559Z",
     "iopub.status.idle": "2025-02-13T12:46:37.715585Z",
     "shell.execute_reply": "2025-02-13T12:46:37.714831Z"
    },
    "papermill": {
     "duration": 0.015823,
     "end_time": "2025-02-13T12:46:37.716781",
     "exception": false,
     "start_time": "2025-02-13T12:46:37.700958",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = Path(\"./Dataset\")  \n",
    "model_dir = Path(\"./Models\") \n",
    "model_dir.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8d4f41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-13T12:46:37.738208Z",
     "iopub.status.busy": "2025-02-13T12:46:37.737930Z",
     "iopub.status.idle": "2025-02-13T12:46:40.665761Z",
     "shell.execute_reply": "2025-02-13T12:46:40.665029Z"
    },
    "papermill": {
     "duration": 2.940229,
     "end_time": "2025-02-13T12:46:40.667236",
     "exception": false,
     "start_time": "2025-02-13T12:46:37.727007",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Custom transformation class for integrating Albumentations with Fastai\n",
    "class AlbumentationsTransform(RandTransform):\n",
    "    split_idx = 0  # Indicates whether it's training (0) or validation (1) phase\n",
    "    order = 2  # Set the order in which transformations are applied\n",
    "\n",
    "    def __init__(self, train_aug, valid_aug):\n",
    "        self.train_aug = train_aug  # Store training augmentations\n",
    "        self.valid_aug = valid_aug  # Store validation augmentations\n",
    "\n",
    "    def before_call(self, b, split_idx):\n",
    "        self.idx = split_idx  # Assign split index to apply the correct augmentations\n",
    "\n",
    "    def encodes(self, img: PILImage):\n",
    "        img_np = np.array(img)  # Convert image to numpy array for Albumentations processing\n",
    "        # Apply training augmentations if idx is 0 (training phase), otherwise apply validation augmentations\n",
    "        aug_img = self.train_aug(image=img_np)['image'] if self.idx == 0 else self.valid_aug(image=img_np)['image']\n",
    "        return PILImage.create(aug_img)  # Convert augmented numpy array back to PILImage\n",
    "\n",
    "# Define training augmentations with several transformations for robustness\n",
    "def get_train_aug(sz):\n",
    "    return A.Compose([\n",
    "        A.RandomResizedCrop(height=sz, width=sz, scale=(0.08, 1.0), p=1),  # Random crop to focus on varied portions of images\n",
    "        A.Transpose(p=0.5),  # Random transpose (flipping) to introduce variety\n",
    "        A.HorizontalFlip(p=0.5),  # Horizontal flip for rotational invariance\n",
    "        A.VerticalFlip(p=0.5),  # Vertical flip for additional variety\n",
    "        A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5),  # Shift, scale, rotate for more transformations\n",
    "        A.HueSaturationValue(hue_shift_limit=0.2, p=0.5),  # Random color adjustments for robustness\n",
    "        A.RandomBrightnessContrast(brightness_limit=(-0.1, 0.1), contrast_limit=(-0.1, 0.1), p=0.5),  # Random brightness and contrast changes\n",
    "        A.CoarseDropout(max_holes=8, max_height=32, max_width=32, p=0.5),  # Introduce occlusions to help generalize\n",
    "    ])\n",
    "\n",
    "# Define validation augmentations (only resizing images for evaluation consistency)\n",
    "def get_valid_aug(sz):\n",
    "    return A.Compose([\n",
    "        A.Resize(height=sz, width=sz, interpolation=cv2.INTER_LINEAR, p=1.0)  # Resize all validation images to the desired size\n",
    "    ])\n",
    "\n",
    "# Create data loaders with augmentation and normalization for training and validation\n",
    "def get_dls(path, sz=224, bs=64):\n",
    "    item_tfms = [Resize(sz), AlbumentationsTransform(get_train_aug(sz), get_valid_aug(sz))]  # Apply resizing and augmentation\n",
    "    batch_tfms = [Normalize.from_stats(*imagenet_stats)]  # Normalize using ImageNet statistics for pre-trained models\n",
    "\n",
    "    # Load images from folder, apply item transformations, and split data into training/validation sets\n",
    "    dls = ImageDataLoaders.from_folder(\n",
    "        path, valid_pct=0.2, seed=42,  # Use 20% for validation\n",
    "        item_tfms=item_tfms, batch_tfms=batch_tfms, bs=bs  # Apply item and batch transformations\n",
    "    )\n",
    "    return dls\n",
    "\n",
    "print(\"Preparing data...\")\n",
    "dls = get_dls(path, sz=224, bs=64)  # Create data loaders with image size 224 and batch size 64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cb16c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-13T12:46:40.688824Z",
     "iopub.status.busy": "2025-02-13T12:46:40.688502Z",
     "iopub.status.idle": "2025-02-13T12:46:40.696171Z",
     "shell.execute_reply": "2025-02-13T12:46:40.695362Z"
    },
    "papermill": {
     "duration": 0.019776,
     "end_time": "2025-02-13T12:46:40.697487",
     "exception": false,
     "start_time": "2025-02-13T12:46:40.677711",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to print the classification report for the trained model\n",
    "def print_classification_report(learn):\n",
    "    preds, targs = learn.get_preds(dl=learn.dls.valid)  # Get predictions and targets from the validation set\n",
    "    pred_classes = preds.argmax(dim=1)  # Get the predicted class (highest probability)\n",
    "    class_names = learn.dls.vocab  # Get class names from the data loader\n",
    "    print(classification_report(targs, pred_classes, target_names=class_names))  # Print the classification report\n",
    "\n",
    "# Function to train the model with specified parameters\n",
    "def train_model(model_name, arch, dls, epochs=10):\n",
    "    print(f\"\\nTraining {model_name}...\")\n",
    "\n",
    "    # Create a learner object with the given data loader, architecture, and metrics\n",
    "    learn = vision_learner(dls, arch, metrics=[accuracy], wd=1e-2,\n",
    "                          cbs=[EarlyStoppingCallback(monitor='valid_loss', patience=2),\n",
    "                               SaveModelCallback(monitor='valid_loss')])  # Callbacks for early stopping and saving model\n",
    "\n",
    "    learn.model_dir = model_dir.absolute()  # Set the directory for saving the model\n",
    "\n",
    "    # Phase 1: Fine-tuning the model\n",
    "    print(\"=\"*50)\n",
    "    print(\"Phase 1: Fine-tuning...\")\n",
    "    print(\"=\"*50)\n",
    "    learn.fine_tune(epochs)  # Fine-tune the model on the data\n",
    "\n",
    "    # Phase 2: Training with unfrozen layers (train all layers)\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Phase 2: Training with unfrozen layers...\")\n",
    "    print(\"=\"*50)\n",
    "    learn.unfreeze()  # Unfreeze all layers of the model for further training\n",
    "\n",
    "    # Find the best learning rate using learning rate finder\n",
    "    try:\n",
    "        lr_results = learn.lr_find(suggest_funcs=(steep, valley))  # Find steepest and valley points of the loss curve\n",
    "        lr_min = lr_results[0] if isinstance(lr_results, tuple) else 1e-3  # Get the suggested minimum learning rate\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Failed to find learning rate. Using default 1e-3. Error: {e}\")\n",
    "        lr_min = 1e-3  # Default learning rate in case of failure\n",
    "\n",
    "    # Training the model with the best learning rate found\n",
    "    learn.fit_one_cycle(epochs, lr_min)  # Train for the given number of epochs with learning rate schedule\n",
    "\n",
    "    # Evaluation phase: Generate confusion matrix and top losses for inspection\n",
    "    print(\"\\nGenerating evaluation metrics...\")\n",
    "    interp = ClassificationInterpretation.from_learner(learn)  # Get model interpretation for evaluation\n",
    "    interp.plot_confusion_matrix()  # Plot confusion matrix to visualize model performance\n",
    "    interp.plot_top_losses(10, nrows=2)  # Plot the top 10 worst predictions\n",
    "\n",
    "    # Print the classification report for further insights\n",
    "    print(f\"\\nClassification Report for {model_name}:\")\n",
    "    print_classification_report(learn)\n",
    "\n",
    "    # Get the final accuracy and convert it to percentage\n",
    "    acc = learn.validate()[1] * 100\n",
    "    print(f\"\\nFinal Accuracy for {model_name}: {acc:.2f}%\")\n",
    "\n",
    "    # Save the model only if accuracy is greater than 90%\n",
    "    if acc > 90:\n",
    "        model_path = model_dir / f'{model_name}.pkl'  # Save model as a .pkl file\n",
    "        learn.export(model_path)  # Export the trained model\n",
    "        print(f\"Model saved as {model_path}\")  # Print confirmation of saving the model\n",
    "        return learn  # Return the trained model\n",
    "    else:\n",
    "        print(f\"Model {model_name} not saved (Accuracy < 90%)\")  # Do not save model if accuracy is less than 90%\n",
    "        return None  # Return None if the model is not saved\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90f1c87",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-13T12:46:40.718872Z",
     "iopub.status.busy": "2025-02-13T12:46:40.718587Z",
     "iopub.status.idle": "2025-02-13T13:30:31.282330Z",
     "shell.execute_reply": "2025-02-13T13:30:31.281353Z"
    },
    "papermill": {
     "duration": 2630.584579,
     "end_time": "2025-02-13T13:30:31.292354",
     "exception": false,
     "start_time": "2025-02-13T12:46:40.707775",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "models_to_train = [\n",
    "    (\"resnet50\", resnet50, \"hair-resnet50\"),\n",
    "    (\"vgg16_bn\", vgg16_bn, \"hair-vgg16\"),\n",
    "    (\"convnext_tiny\", convnext_tiny, \"hair-convnext-tiny\")\n",
    "]\n",
    "\n",
    "trained_models = {}\n",
    "\n",
    "for model_desc, arch, save_name in models_to_train:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Starting training for {model_desc}\")\n",
    "    print(f\"{'='*50}\")\n",
    "\n",
    "    model = train_model(save_name, arch, dls)\n",
    "    if model:\n",
    "        trained_models[model_desc] = model\n",
    "        print(f\"\\nCompleted training {model_desc}\")\n",
    "    else:\n",
    "        print(f\"\\nSkipping {model_desc} for ensemble\")\n",
    "\n",
    "print(\"\\nTraining completed for all models!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf975c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-13T13:30:31.419865Z",
     "iopub.status.busy": "2025-02-13T13:30:31.419558Z",
     "iopub.status.idle": "2025-02-13T13:30:46.643959Z",
     "shell.execute_reply": "2025-02-13T13:30:46.643007Z"
    },
    "papermill": {
     "duration": 15.289352,
     "end_time": "2025-02-13T13:30:46.645226",
     "exception": false,
     "start_time": "2025-02-13T13:30:31.355874",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ensemble_predict(models, dls):\n",
    "    preds_list = []\n",
    "\n",
    "    for model_name, learn in models.items():\n",
    "        preds, _ = learn.get_preds(dl=dls.valid)\n",
    "        preds_list.append(preds)\n",
    "\n",
    "    avg_preds = torch.mean(torch.stack(preds_list), dim=0)\n",
    "    final_pred_classes = avg_preds.argmax(dim=1)\n",
    "\n",
    "    return final_pred_classes\n",
    "\n",
    "if trained_models:\n",
    "    print(\"\\nGenerating Ensemble Predictions...\")\n",
    "    ensemble_preds = ensemble_predict(trained_models, dls)\n",
    "\n",
    "    # Compute Classification Report\n",
    "    _, targs = list(trained_models.values())[0].get_preds(dl=dls.valid)\n",
    "    class_names = dls.vocab\n",
    "    print(\"\\nEnsemble Model Classification Report:\")\n",
    "    print(classification_report(targs, ensemble_preds, target_names=class_names))\n",
    "else:\n",
    "    print(\"\\nNo models were trained with accuracy > 90%, skipping ensemble.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be8c601",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import fastai.vision.all as fv\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Load the FastAI model\n",
    "model_path = \"Models/hair-vgg16.pkl\"\n",
    "learn = fv.load_learner(model_path)\n",
    "\n",
    "# Define transformation (FastAI handles its own preprocessing)\n",
    "class_labels = [\"Straight\", \"Wavy\", \"Curly\", \"Dreadlocks\", \"Kinky\"]\n",
    "\n",
    "# Start webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to capture image\")\n",
    "        break\n",
    "\n",
    "    # Convert OpenCV frame to PIL Image\n",
    "    img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    img_pil = Image.fromarray(img)\n",
    "\n",
    "    # Run inference with FastAI\n",
    "    pred, pred_idx, probs = learn.predict(img_pil)\n",
    "    predicted_label = str(pred)\n",
    "\n",
    "    # Display prediction on webcam feed\n",
    "    cv2.putText(frame, f\"Prediction: {predicted_label}\", (50, 50),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "    cv2.imshow(\"Webcam - Hair Type Classification\", frame)\n",
    "\n",
    "    # Check if the window is closed\n",
    "    if cv2.getWindowProperty(\"Webcam - Hair Type Classification\", cv2.WND_PROP_VISIBLE) < 1:\n",
    "        print(\"Window closed, stopping camera...\")\n",
    "        break\n",
    "\n",
    "    # Press 'q' to exit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5737179,
     "sourceId": 9440976,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6660170,
     "sourceId": 10740461,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30887,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2680.735131,
   "end_time": "2025-02-13T13:30:49.268785",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-02-13T12:46:08.533654",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
