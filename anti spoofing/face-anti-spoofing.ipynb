{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Imports\n\nimport numpy as np  # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nimport shutil # high-level operations on files\nfrom tqdm import tqdm # Progress bar and status logging\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report,confusion_matrix\n\nimport cv2 # computer vision algorithms\n\n# Importing the Keras libraries and packages\nfrom keras import utils\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D\nfrom keras.layers import MaxPooling2D\nfrom keras.layers import Flatten\nfrom keras.layers import Dense\nfrom keras.layers import Dropout","metadata":{"execution":{"iopub.status.busy":"2023-03-17T12:58:31.876165Z","iopub.execute_input":"2023-03-17T12:58:31.876466Z","iopub.status.idle":"2023-03-17T12:58:34.732103Z","shell.execute_reply.started":"2023-03-17T12:58:31.876411Z","shell.execute_reply":"2023-03-17T12:58:34.731229Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"Using TensorFlow backend.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Input data files are available in the \"../input/\" directory.\n\nfor dirname, _, filenames in os.walk('/kaggle'):\n    print (dirname)\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-03-17T12:58:34.734011Z","iopub.execute_input":"2023-03-17T12:58:34.734282Z","iopub.status.idle":"2023-03-17T12:58:37.421310Z","shell.execute_reply.started":"2023-03-17T12:58:34.734236Z","shell.execute_reply":"2023-03-17T12:58:37.420478Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle\n/kaggle/lib\n/kaggle/lib/kaggle\n/kaggle/input\n/kaggle/input/real-and-fake-face-detection\n/kaggle/input/real-and-fake-face-detection/real_and_fake_face\n/kaggle/input/real-and-fake-face-detection/real_and_fake_face/training_fake\n/kaggle/input/real-and-fake-face-detection/real_and_fake_face/training_real\n/kaggle/input/real-and-fake-face-detection/real_and_fake_face_detection\n/kaggle/input/real-and-fake-face-detection/real_and_fake_face_detection/real_and_fake_face\n/kaggle/input/real-and-fake-face-detection/real_and_fake_face_detection/real_and_fake_face/training_fake\n/kaggle/input/real-and-fake-face-detection/real_and_fake_face_detection/real_and_fake_face/training_real\n/kaggle/working\n","output_type":"stream"}]},{"cell_type":"code","source":"# Configuration\n\nDATASET_DIR = '/kaggle/input/real-and-fake-face-detection/real_and_fake_face_detection/real_and_fake_face'\nTRAIN_DIR = '/kaggle/train_dataset'\nTEST_DIR = '/kaggle/test_dataset'\n\nRATE = 0.2 # splitting proportion for training and test datasets\n\n# Parameters for Grid Search\n\nN_EPOCHS = [20] #[20, 40, 100, 200]\nOPTIMIZERS = ['adam'] #['adam', 'rmsprop', 'SGD']\nDROPOUT_RATES =  [0.1, 0.2, 0.4]\nLOSS_FUNCTIONS = ['binary_crossentropy']  #['sparse_categorical_crossentropy', 'kullback_leibler_divergence']    ","metadata":{"execution":{"iopub.status.busy":"2023-03-17T12:58:37.422586Z","iopub.execute_input":"2023-03-17T12:58:37.423032Z","iopub.status.idle":"2023-03-17T12:58:37.428889Z","shell.execute_reply.started":"2023-03-17T12:58:37.422982Z","shell.execute_reply":"2023-03-17T12:58:37.427677Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"os.mkdir(TRAIN_DIR)\nos.mkdir(TRAIN_DIR+'/fake')\nos.mkdir(TRAIN_DIR+'/real')\n\nos.mkdir(TEST_DIR)\nos.mkdir(TEST_DIR+'/fake')\nos.mkdir(TEST_DIR+'/real')","metadata":{"execution":{"iopub.status.busy":"2023-03-17T12:58:37.430248Z","iopub.execute_input":"2023-03-17T12:58:37.430759Z","iopub.status.idle":"2023-03-17T12:58:37.439969Z","shell.execute_reply.started":"2023-03-17T12:58:37.430638Z","shell.execute_reply":"2023-03-17T12:58:37.439322Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"Updated folder structure:","metadata":{}},{"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle'):\n    print (dirname)","metadata":{"execution":{"iopub.status.busy":"2023-03-17T12:58:37.443658Z","iopub.execute_input":"2023-03-17T12:58:37.444140Z","iopub.status.idle":"2023-03-17T12:58:37.467113Z","shell.execute_reply.started":"2023-03-17T12:58:37.443959Z","shell.execute_reply":"2023-03-17T12:58:37.466387Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"/kaggle\n/kaggle/train_dataset\n/kaggle/train_dataset/fake\n/kaggle/train_dataset/real\n/kaggle/lib\n/kaggle/lib/kaggle\n/kaggle/test_dataset\n/kaggle/test_dataset/fake\n/kaggle/test_dataset/real\n/kaggle/input\n/kaggle/input/real-and-fake-face-detection\n/kaggle/input/real-and-fake-face-detection/real_and_fake_face\n/kaggle/input/real-and-fake-face-detection/real_and_fake_face/training_fake\n/kaggle/input/real-and-fake-face-detection/real_and_fake_face/training_real\n/kaggle/input/real-and-fake-face-detection/real_and_fake_face_detection\n/kaggle/input/real-and-fake-face-detection/real_and_fake_face_detection/real_and_fake_face\n/kaggle/input/real-and-fake-face-detection/real_and_fake_face_detection/real_and_fake_face/training_fake\n/kaggle/input/real-and-fake-face-detection/real_and_fake_face_detection/real_and_fake_face/training_real\n/kaggle/working\n","output_type":"stream"}]},{"cell_type":"code","source":"# Split folders with images into training, validation and test folders.\n# OPTION 1 (using split-folders)\n\n#pip install split-folders\n\n#import split_folders\n\n# Split with a ratio.\n# To only split into training and validation set, set a tuple to `ratio`, i.e, `(.8, .2)`.\n#split_folders.ratio('input_folder', output=\"output\", seed=1337, ratio=(.8, .1, .1)) # default values\n","metadata":{"execution":{"iopub.status.busy":"2023-03-17T12:58:37.468854Z","iopub.execute_input":"2023-03-17T12:58:37.469114Z","iopub.status.idle":"2023-03-17T12:58:37.472665Z","shell.execute_reply.started":"2023-03-17T12:58:37.469072Z","shell.execute_reply":"2023-03-17T12:58:37.471895Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Split image files into test and training set \n# OPTION 2 (copying files into newly created folders)\nfiles_real = os.listdir(f'{DATASET_DIR}/training_real')\nfiles_fake = os.listdir(f'{DATASET_DIR}/training_fake')\n\n\n# sample from each class to create a test set\nnp.random.seed(0)\nfiles_real_test = np.random.choice(\n    files_real,\n    size=round(len(files_real) * RATE),\n    replace=False,\n    p=None)\n\nfiles_real_train = list(set(files_real) - set(files_real_test)) #[file for file in files_real if file not in files_real_test] \n\nfiles_fake_test = np.random.choice(\n    files_fake,\n    size=round(len(files_fake) * RATE),\n    replace=False,\n    p=None)\n\nfiles_fake_train = list(set(files_fake) - set(files_fake_test)) #[file for file in files_fake if file not in files_fake_test] \n\nfor file in files_real_train:\n    shutil.copyfile(DATASET_DIR+'/training_real/'+file, TRAIN_DIR+'/real/'+file) \n\nfor file in files_fake_train:\n    shutil.copyfile(DATASET_DIR+'/training_fake/'+file, TRAIN_DIR+'/fake/'+file) \n\nfor file in files_real_test:\n    shutil.copyfile(DATASET_DIR+'/training_real/'+file, TEST_DIR+'/real/'+file) \n\nfor file in files_fake_test:\n    shutil.copyfile(DATASET_DIR+'/training_fake/'+file, TEST_DIR+'/fake/'+file) \n    ","metadata":{"execution":{"iopub.status.busy":"2023-03-17T12:58:37.474032Z","iopub.execute_input":"2023-03-17T12:58:37.474593Z","iopub.status.idle":"2023-03-17T12:58:52.174384Z","shell.execute_reply.started":"2023-03-17T12:58:37.474517Z","shell.execute_reply":"2023-03-17T12:58:52.173439Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"train_samples = sum([len(files) for r, d, files in os.walk(TRAIN_DIR)])\ntest_samples = sum([len(files) for r, d, files in os.walk(TEST_DIR)])\nprint('Number of training images: {} \\nNumber of test images: {}'.format(train_samples, test_samples))","metadata":{"execution":{"iopub.status.busy":"2023-03-17T12:58:52.178067Z","iopub.execute_input":"2023-03-17T12:58:52.178323Z","iopub.status.idle":"2023-03-17T12:58:52.197134Z","shell.execute_reply.started":"2023-03-17T12:58:52.178271Z","shell.execute_reply":"2023-03-17T12:58:52.195314Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Number of training images: 1633 \nNumber of test images: 408\n","output_type":"stream"}]},{"cell_type":"code","source":"# load and show an image with Pillow\n# from PIL import Image\n# image = Image.open('/kaggle/test_dataset/fake/hard_39_1111.jpg')\n# # some details about the image\n# print(image.format)\n# print(image.mode)\n# print(image.size)","metadata":{"execution":{"iopub.status.busy":"2023-03-17T12:58:52.198560Z","iopub.execute_input":"2023-03-17T12:58:52.199018Z","iopub.status.idle":"2023-03-17T12:58:52.321585Z","shell.execute_reply.started":"2023-03-17T12:58:52.198966Z","shell.execute_reply":"2023-03-17T12:58:52.320522Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"\ndef get_images(path, img_shape=(64, 64)):\n \n    '''\n    Returns a np array of images and labels from path\n    Images must be stored in path/class1, path/class2\n    '''\n    main_path = path\n    y = []\n    list = [name for name in os.listdir(main_path) if os.path.isdir(os.path.join(main_path, name))]\n    print(list)\n    image_collection = []\n    for idx,folder in enumerate(list):\n \n        label = idx\n        \n        sub_list = sorted(os.listdir(os.path.join(main_path,folder)))\n \n        for i in tqdm(range(1, len(sub_list))):\n            image_path = os.path.join(main_path, folder, sub_list[i])\n            read_image = cv2.imread(image_path)\n            image_resized = cv2.resize(read_image, img_shape, interpolation=cv2.INTER_AREA)\n \n            image = np.float32(image_resized)\n            image = cv2.normalize(image, None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F) #Change alpha, beta according to the preprocessing you desire\n            \n            image_collection.append(image)\n            \n            y.append(label)\n \n    y = np.array(y)\n    y = utils.to_categorical(y,num_classes=len(list))\n \n    return image_collection, y[:,0] \n","metadata":{"execution":{"iopub.status.busy":"2023-03-17T12:58:52.323050Z","iopub.execute_input":"2023-03-17T12:58:52.323492Z","iopub.status.idle":"2023-03-17T12:58:52.335859Z","shell.execute_reply.started":"2023-03-17T12:58:52.323309Z","shell.execute_reply":"2023-03-17T12:58:52.334706Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Preparing test and trainng datasets\nX_train,y_train = get_images(TRAIN_DIR,img_shape=(64,64))\nX_test,y_test = get_images(TEST_DIR,img_shape=(64,64))\nX_train = np.array(X_train)\nX_test = np.array(X_test)\n# print(X_train.shape)\n# print(X_train[0])\n# from PIL import Image\n# im = Image.fromarray(X_train[0].astype('uint8'))\n# im.save(\"img50.jpg\")\n","metadata":{"execution":{"iopub.status.busy":"2023-03-17T12:58:52.337228Z","iopub.execute_input":"2023-03-17T12:58:52.337717Z","iopub.status.idle":"2023-03-17T12:59:17.184606Z","shell.execute_reply.started":"2023-03-17T12:58:52.337511Z","shell.execute_reply":"2023-03-17T12:59:17.183823Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"  0%|          | 0/767 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"['fake', 'real']\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 767/767 [00:09<00:00, 79.10it/s] \n100%|██████████| 864/864 [00:10<00:00, 84.88it/s]\n  5%|▍         | 9/191 [00:00<00:02, 84.87it/s]","output_type":"stream"},{"name":"stdout","text":"['fake', 'real']\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 191/191 [00:02<00:00, 88.30it/s]\n100%|██████████| 215/215 [00:02<00:00, 79.40it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"print('Training set', X_train.shape)\nprint('Test set', X_test.shape)","metadata":{"execution":{"iopub.status.busy":"2023-03-17T12:59:17.185896Z","iopub.execute_input":"2023-03-17T12:59:17.186348Z","iopub.status.idle":"2023-03-17T12:59:17.192260Z","shell.execute_reply.started":"2023-03-17T12:59:17.186297Z","shell.execute_reply":"2023-03-17T12:59:17.191513Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Training set (1631, 64, 64, 3)\nTest set (406, 64, 64, 3)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"We don't have too much data to train the network. \nOne of possible workarounds is to use ImageDataGenerator.\nOn the one hand, it does allow us to generate additional examples. On the other hand, all of these examples are based on a too small dataset and the network still cannot generalize to data it was never trained on \n","metadata":{}},{"cell_type":"code","source":"# from keras.preprocessing.image import ImageDataGenerator\n# train_datagen = ImageDataGenerator(shear_range = 0.2,\n#                                    zoom_range = 0.2,\n#                                    horizontal_flip = True)\n# batches = 0\n# for _ in train_datagen.flow_from_directory(directory=TRAIN_DIR, class_mode='binary', batch_size = 32, save_prefix='real_aug', save_to_dir=TRAIN_DIR+'/real', save_format='jpeg'):\n#     batches += 1\n#     if batches >= number_of_real_images / 32:\n#         break # to stop the indefinite generator loop\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-03-17T12:59:17.193461Z","iopub.execute_input":"2023-03-17T12:59:17.193916Z","iopub.status.idle":"2023-03-17T12:59:17.202687Z","shell.execute_reply.started":"2023-03-17T12:59:17.193866Z","shell.execute_reply":"2023-03-17T12:59:17.201946Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"#Shuffle training examples\nX_train, y_train = shuffle(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2023-03-17T12:59:17.204021Z","iopub.execute_input":"2023-03-17T12:59:17.204694Z","iopub.status.idle":"2023-03-17T12:59:17.271039Z","shell.execute_reply.started":"2023-03-17T12:59:17.204487Z","shell.execute_reply":"2023-03-17T12:59:17.270143Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def build_classifier(optimizer, dropout, loss):\n    classifier = Sequential() # Initialising the CNN    \n    classifier.add(Conv2D(32, (3, 3), input_shape = (64, 64, 3), activation = 'relu')) \n    classifier.add(MaxPooling2D(pool_size = (2, 2))) \n    classifier.add(Dropout(dropout))\n    classifier.add(Conv2D(32, (3, 3), activation = 'relu'))  \n    classifier.add(MaxPooling2D(pool_size = (2, 2)))\n    classifier.add(Dropout(dropout))\n    classifier.add(Conv2D(32, (3, 3), activation = 'relu'))  \n    classifier.add(MaxPooling2D(pool_size = (2, 2)))\n    classifier.add(Dropout(dropout))\n    classifier.add(Flatten())\n    classifier.add(Dense(units = 128, activation = 'relu'))\n    classifier.add(Dense(units = 1, activation = 'sigmoid')) #'tanh'))\n    \n    classifier.compile(optimizer = optimizer, loss = loss, metrics = ['accuracy'])\n    \n    return classifier\n\nclassifier = KerasClassifier(build_fn = build_classifier)\n\ngrid_parameters = {'epochs': N_EPOCHS,\n                  'optimizer': OPTIMIZERS,\n                  'dropout': DROPOUT_RATES,                  \n                  'loss':LOSS_FUNCTIONS                        \n                  }\n\n\ngrid_search = GridSearchCV(estimator = classifier,\n                           param_grid = grid_parameters,\n                           scoring = 'accuracy',\n                           cv = 10)\n\n\ngrid_search = grid_search.fit(X_train, y_train, verbose=0)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-03-17T12:59:17.272723Z","iopub.execute_input":"2023-03-17T12:59:17.273502Z","iopub.status.idle":"2023-03-17T13:04:15.344388Z","shell.execute_reply.started":"2023-03-17T12:59:17.273078Z","shell.execute_reply":"2023-03-17T13:04:15.343451Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"best_parameters = grid_search.best_params_\nbest_accuracy = grid_search.best_score_\nprint(best_parameters)\nprint(best_accuracy)","metadata":{"execution":{"iopub.status.busy":"2023-03-17T13:04:15.345805Z","iopub.execute_input":"2023-03-17T13:04:15.346081Z","iopub.status.idle":"2023-03-17T13:04:15.352682Z","shell.execute_reply.started":"2023-03-17T13:04:15.346037Z","shell.execute_reply":"2023-03-17T13:04:15.351860Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"{'dropout': 0.1, 'epochs': 20, 'loss': 'binary_crossentropy', 'optimizer': 'adam'}\n0.6082158185162477\n","output_type":"stream"}]},{"cell_type":"code","source":"predicted = grid_search.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2023-03-17T13:04:15.354085Z","iopub.execute_input":"2023-03-17T13:04:15.354528Z","iopub.status.idle":"2023-03-17T13:04:16.771667Z","shell.execute_reply.started":"2023-03-17T13:04:15.354478Z","shell.execute_reply":"2023-03-17T13:04:16.770818Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"\nprint('Confusion matrix for training set:')\nprint(confusion_matrix(y_train,grid_search.predict(X_train)))\nprint('\\n')\nprint(classification_report(y_train,grid_search.predict(X_train)))\n\nprint('Confusion matrix  for test set:')\nprint(confusion_matrix(y_test,predicted))\nprint('\\n')\nprint(classification_report(y_test,predicted))","metadata":{"execution":{"iopub.status.busy":"2023-03-17T13:04:16.774795Z","iopub.execute_input":"2023-03-17T13:04:16.775104Z","iopub.status.idle":"2023-03-17T13:04:17.067911Z","shell.execute_reply.started":"2023-03-17T13:04:16.775043Z","shell.execute_reply":"2023-03-17T13:04:17.067235Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Confusion matrix for training set:\n[[810  54]\n [ 82 685]]\n\n\n              precision    recall  f1-score   support\n\n         0.0       0.91      0.94      0.92       864\n         1.0       0.93      0.89      0.91       767\n\n    accuracy                           0.92      1631\n   macro avg       0.92      0.92      0.92      1631\nweighted avg       0.92      0.92      0.92      1631\n\nConfusion matrix  for test set:\n[[138  77]\n [ 92  99]]\n\n\n              precision    recall  f1-score   support\n\n         0.0       0.60      0.64      0.62       215\n         1.0       0.56      0.52      0.54       191\n\n    accuracy                           0.58       406\n   macro avg       0.58      0.58      0.58       406\nweighted avg       0.58      0.58      0.58       406\n\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}